---
title: "Why Risk it, When You Can {rix} it: A Tutorial for Computational Reproducibility Focused on Simulation Studies"
shorttitle: "Reproducibility with rix"
author:
  - name: Felipe Fontana Vieira
    corresponding: true
    email: felipe.fontanavieira@ugent.be
    affiliations:
      - name: Ghent University 
        department: Department of Data Analysis 
  - name: Jason Geller
    affiliations:
      - name: Boston College
        department: Department of Psychology and Neuroscience
  - name: Bruno Rodrigues
    affiliations:
      - name: Ministry of Research and Higher Education, Luxembourg
        department: Statistics and Data Strategy Departments
abstract: |
  Reproducibility remains limited in psychology, in part because reproducibility
  exists on a spectrum -- from sharing isolated code fragments to providing
  fully executable pipelines that ensure identical results. This article
  introduces Nix and the {rix} R package as a way to provide a comprehensive
  solution for achieving full computational reproducibility in simulation
  studies. Building on this, we also demonstrate a tutorial on how to use {rix}
  to obtain a reproducible manuscript using the apaquarto extension.

keywords: [reproducibility, Nix, simulation studies, R, computational methods]
word-count: true
authornote: ~
format:
  apaquarto-pdf:
    document-mode: man
    keep-tex: true
    include-in-header:
      text: |
        \usepackage{setspace}
        \usepackage{etoolbox}
        \raggedbottom
        % Single-space code blocks
        \AtBeginEnvironment{Shaded}{\singlespacing}
        \AtBeginEnvironment{verbatim}{\singlespacing}
  apaquarto-html: default

bibliography: references.bib
---

```{r}
#| label: load-packages
#| echo: false
#| message: false

# required packages
library(marginaleffects)
library(simhelpers)
library(rvinecopulib)
library(doParallel)
library(doRNG)
library(ggplot2)
library(cowplot)
library(knitr)
library(dplyr)

```

Psychological science is in the midst of a credibility revolution, which has
prompted substantial progress in how research is conducted and evaluated
[@vazire2018]. Yet, despite notable progress, a key cornerstone of science,
reproducibility (i.e., the ability to precisely reproduce the results of a study
or studies based on provided data, code, materials, and software/hardware)
remains limited [@hardwicke2020]. As a result, ensuring reproducibility remains
an open and pressing challenge for psychological science.

Addressing this gap is complicated by the fact that reproducibility is not a
binary feature but instead exists along a continuum [@peng_2011]. At the lower
end, reproducibility may be interpreted as sharing only a manuscript. Further
along the spectrum, it may involve providing partial code, complete analysis
scripts, or publicly accessible datasets. At the highest level, reproducibility
entails documenting a fully specified computational environment that allows
others to recreate identical results—from raw data to final manuscript
output—with minimal friction. As a result, researchers may implicitly target
different points on this continuum and efforts to improve reproducibility can
diverge substantially in both goals and implementation.

Open science initiatives have made considerable progress in encouraging movement
along this continuum. For example, journals have begun offering open-science
badges [@kidwell_etall_2016] and platforms like the Open Science Framework (OSF)
have created "challenges" to make data and code sharing increasingly routine
[@levenstein_lyle_2018]. However, these efforts largely occupy the lower and
middle portions of the continuum, emphasizing what is shared rather than how
shared materials can be executed in practice.

Data and code are never fully self-sufficient to reproduce a set of findings.
Assuming the data and code are error-free, reproducibility depends on a
hierarchy of software components—collectively referred to as
*dependencies*—including the programming language version, the packages used in
the analysis, and the system libraries on which those packages rely. When these
dependencies differ from those used in the original analysis, code may fail,
behave inconsistently across machines, or yield conflicting numerical results
[@baker_etall_2024; @hodges_etall_2023; @glatard_etall_2015;
@nosek_etall_2022].These issues are particularly acute for simulation studies,
which rely on complex codebases, versioned dependencies, and intricate software
configurations [@luijken_etall_2024; @siepe_etall_2024].

To make this concrete, we use *computational environment* to refer to the
complete software context required for an analysis to run successfully—the
programming language version, package versions, system libraries, and operating
system [@rodrigues_2023; @rodrigues_baumann_2026_polyglot]. We define
*computational environment reproducibility* as the ability to reconstruct this
entire set of software dependencies on any machine and at any future time, such
that executing the same code yields the same numerical results. Empirical
assessments show that current practice falls short of this ideal.
@siepe_etall_2024 report that nearly two-thirds of simulation studies in
psychology provide no accompanying code, and among those that do, documentation
of the computational environment is rarely included. This gap is consequential:
simulation studies inform methodological recommendations, meaning that
insufficient reproducibility undermines confidence in those recommendations
[@luijken_etall_2024; @white_etall_2024].

Arguably, these challenges persist because researchers must navigate a
fragmented landscape of solutions, each addressing only part of the problem.
Package-level managers such as {renv} [@ushey_2024] and {groundhog}
[@simonsohn_2020] stabilize R package versions but do not manage the R
interpreter itself or the system-level libraries those packages depend on.
Workflow orchestration tools such as {targets} [@landau_2021] and Make
[@feldman_1979] support reproducibility in a different sense: they specify the
structure of an analysis by formalizing the order in which steps should run and
by tracking dependencies among intermediate results. These tools clarify *how*
an analysis proceeds, but they assume that the software stack required to run
each step is already stable. Containerization tools such as Docker, including
R-focused implementations like Rocker[@boettiger_2015;
@boettiger_eddelbuettel_2017] offer a more comprehensive approach by bundling
the full environment—operating system, system libraries, interpreter versions,
and packages—into a single executable image. Yet their use requires familiarity
with Linux system administration, and even containerization may suffer from
temporal drift when Dockerfiles rely on mutable upstream repositories
[@malka2024]. For a detailed comparison of these tools and their limitations,
see @rodrigues_baumann_2026_polyglot. Researchers thus face a difficult choice
between solutions that are accessible but incomplete or approaches that are
powerful but demand substantial technical expertise.

In this article, therefore, we focus specifically on computational environment
reproducibility as the foundation upon which other reproducibility practices
depend. For that, we introduce Nix [@dolstra_etall_2004], a functional software
ecosystem designed to make software installation deterministic, and {rix}
[@rodrigues_baumann_2025], an R interface that allows researchers to use Nix
without needing deep knowledge of its underlying language or infrastructure. Our
objective is not to introduce a specific workflow orchestration system or to
prescribe a particular analytic structure. Instead, we aim to show how Nix and
{rix} can establish a stable, cross-platform environment within which any
analysis—whether organized in simple, documented script sequences (e.g., .R
files that `source()` others), through more formal orchestration tools (e.g.,
targets) or embedded as code chunks in `.Rmd` or `.qmd` —can be executed
reliably.

We illustrate these ideas through a reproducible simulation study conducted in
R, culminating in an automated APA-formatted manuscript generated with
`apaquarto` [@schneider_2024]. Although the example centers on R because of its
prominence in psychological methodology, the principles underlying environment
reproducibility apply equally to other languages, including Python and Julia,
and to different development environments such as RStudio, VS Code, Emacs, or
Positron. Later in the article, we briefly comment on rixpress [@rixpress],
which extends Nix-based reproducibility to workflows spanning multiple
languages. Throughout, our emphasis remains squarely on the reproducibility of
computational environments as the essential basis for transparent, reliable, and
durable scientific workflows.

# A Practical Example: Setting up a Reproducible Simulation Study with {rix}

Imagine you have just been awarded an extraordinarily prestigious (and
generously funded) grant to conduct a large-scale simulation study. The study is
designed to evaluate the performance of several statistical models under varying
data-generating conditions (see Appendix A for full technical details). 

This tutorial is organized around this scenario. We use a stylized simulation
study to ground the discussion in a concrete and familiar example, mirroring the
structure of a typical methods manuscript. The scenario describes both the
statistical design and the computational workflow one might reasonably encounter
in practice. Although the example is illustrative rather than
substantive—readers can follow the tutorial without engaging deeply with the
simulation details—it provides a tangible reference point for discussing
reproducibility challenges.

In a conventional workflow, simulation studies are typically implemented across
multiple component files, each corresponding to a distinct analytical stage. In
our case, the simulation is organized into five sequential scripts: data
generation (01_data_generation.R), model specification (02_models.R), simulation
execution (03_run_simulation.R), performance metric calculation
(04_performance_metrics.R), and results visualization (05_plots.R). This modular
structure reflects common practice and facilitates development and debugging.

However, because our focus is on the reproducibility of the *entire manuscript*,
we embed all simulation code directly within this document as executable code
chunks in a single .qmd file. When the document is rendered, the simulation runs
from start to finish, producing the reported results and figures automatically.
This approach is not required—and would be impractical for many real-world
simulation studies, which are often too computationally intensive to execute
during manuscript compilation. We return to this trade-off later in the
tutorial.

Now suppose a researcher attempts to reproduce the simulation results reported
in the article. What might prevent them from obtaining identical outcomes? The
natural first concern is package versioning. Installing R packages at a later
time may lead to errors if functions have been renamed or deprecated, or to
subtly different results due to changes in default settings or numerical
implementations. Beyond R packages themselves, many packages rely on
system-level libraries that must be installed separately from R. Our simulation
illustrates this dependency structure directly: the {rvinecopulib} package
interfaces with a C++ backend and links against external libraries such as
Boost, Eigen, and RcppThread \[\@rvinecopulib\].

The R language version introduces another layer of dependency. Code written for
R 4.3 may rely on syntax or functionality that is unavailable in earlier
versions (e.g., R 4.0). More subtly, changes to R’s random number generation
across major versions mean that identical code executed with the same seed can
nevertheless produce different random sequences. For simulation studies—where
specific random draws often underpin reported results—this version sensitivity
is consequential.

Finally, when analyses are embedded in a literate programming workflow—that is,
documents that combine narrative text and executable code—additional layers of
software dependencies arise. For example, rendering R Markdown (`.rmd`) or
Quarto documents (`.qmd`) requires both a document conversion tool (e.g.,
Pandoc, which converts `.rmd` or `.qmd` files into formats such as PDF or HTML)
and a typesetting system such as a LaTeX distribution or Typst. Each of these
components introduces its own versioning constraints and platform-specific
installation requirements. Taken together, these layers highlight that
reproducibility depends not only on code and data, but also on the broader
computational environment in which analyses are executed.

# Nix and {rix}: A Comprehensive Solution

A potential solution to the above issue is Nix [@dolstra_etall_2004]. Nix is a
software ecosystem centered on a purely functional package manager and build
system designed to make software environments reproducible, declarative, and
isolated across platforms (think Apple's or Android's application store). In
practical terms, this means that Nix allows researchers to specify *exactly*
which versions of programming languages, packages, and system libraries an
analysis requires, and to recreate that same environment reliably on another
machine.

Unlike familiar tools such as `install.packages()` in R, `apt-get` on Linux, or
`uv` in Python—which typically manage only a single layer of the software
stack—Nix handles language versions, package versions, and system-level
dependencies within a single framework [@rodrigues_baumann_2025]. Rather than
installing software into shared system directories, Nix builds each environment
as an explicit, self-contained specification. As a result, multiple environments
can coexist without conflict, and analyses can be rerun months or years later
under identical computational conditions.

This unified approach directly addresses the fragmented landscape described
above. Where researchers would otherwise need to coordinate separate tools for
package management, interpreter versions, and system dependencies, Nix brings
all three together within a single declarative model, lowering the barrier to
fully reproducible computational workflows.

## Core Principles

Rather than installing software into global directories (e.g., `/usr/lib`), Nix
places every package in its own directory under `/nix/store`. Each package path
contains a cryptographic hash representing its precise inputs—source code,
dependencies, and build instructions. Because these paths are content-addressed,
multiple versions of the same software can coexist without conflict. A
researcher can, for example, maintain projects requiring R 4.1.0 and R 4.3.3
side by side, or use different package versions across analyses, switching
between them seamlessly [@rodrigues_baumann_2025].

The Nix ecosystem is built around nixpkgs, a version-controlled repository
comprising more than 120,000 packages, including nearly all of CRAN and
Bioconductor. By pinning a specific commit or date, researchers freeze the
entire software stack—R itself, R packages, and all system libraries—at that
point in time. This eliminates the system-dependency problems that tools like
{renv} cannot address [@rodrigues_baumann_2025].

This architecture also ensures stability over time. Empirical work has shown
strong rebuildability and reproducibility rates for historical nixpkgs snapshots
[@rodrigues_baumann_2026_polyglot]. Combined with binary caches, which often
allow environments to materialize in seconds, Nix becomes practical for
interactive research workflows [@rodrigues_baumann_2025].

## The {rix} Package: R Interface to Nix

Nix expressions are written in a dedicated functional language unfamiliar to
most researchers. The {rix} package removes this barrier by providing an
R-native interface. A single call to `rix()` generates complete Nix
configurations from standard R syntax, specifying R versions, CRAN packages,
system libraries, and even Python or Julia components when required. Users never
need to read or write Nix code directly, as {rix} performs the translation
automatically [@rodrigues_baumann_2025].

A key feature of {rix} is its integration with rstats-on-nix, a
community-maintained fork offering daily CRAN snapshots and weekly tested
environments on Linux and macOS. Researchers can request, for example,
`rix(date = "2024-12-14")` to obtain a validated and reproducible environment
without manually assessing compatibility. After the configuration is generated,
`nix_build()` instantiates the environment, and binary caches typically allow
this to complete within seconds [@rodrigues_baumann_2025].

Although Nix is capable of replacing tools like Docker for isolation or {renv}
for package management, it does not require an all-or-nothing transition.
Researchers can adopt it gradually and use it alongside familiar tooling. For
instance, by building Docker images with Nix, converting existing {renv}
lockfiles, or running {targets} pipelines within a Nix-defined environment
[@rodrigues_baumann_2025]. This allows Nix to strengthen reproducibility while
preserving established workflows. For projects requiring more sophisticated
pipeline management, {rixpress} extends Nix’s guarantees to workflow
orchestration, enabling step-level isolation across languages, though such
capabilities lie beyond the present focus on environment reproducibility. We
will come back to this after the tutorial.

## Step I: Installing Nix and {rix}

Before proceeding, both Nix and the {rix} R package need to be installed.
Installation procedures differ across operating systems (Windows via WSL2,
Linux, and macOS), and detailed, up-to-date instructions are maintained in the
official {rix} documentation:

-   **Linux and Windows (WSL2)**:
    <https://docs.ropensci.org/rix/articles/b1-setting-up-and-using-rix-on-linux-and-windows.html>
-   **macOS**:
    <https://docs.ropensci.org/rix/articles/b2-setting-up-and-using-rix-on-macos.html>

Once Nix is installed, {rix} can be installed from CRAN or r from GitHub for the
latest version(@lst-install-rix-cran) [^1]

[^1]: It is worth noting that {rix} can generate Nix expressions even without
    Nix installed on your system—you can write a `default.nix` file without Nix,
    but you cannot build or enter the resulting environment unless Nix is
    installed [@rodrigues_baumann_2025].

```{r}
#| lst-label: lst-install-rix-cran
#| lst-cap: "Installing {rix} from CRAN or developmental version"
#| eval: false
#| echo: true

install.packages("rix") # CRAN

# developmental
install.packages(
  "rix",
  repos = c(
    "https://ropensci.r-universe.dev"
  )
)
```

## Step II: Specifying the Computational Environment {#sec-specifying-environment}

The initial step in establishing a reproducible environment is to create a
script that will generate the environment specification. We recommend creating a
file named `generate_env.R` (or similar) in the project directory. This script
will use the `rix()` function from the {rix} package to produce a `default.nix`
file—a declarative specification that precisely defines all software
dependencies required for the project.

In the case where we use literate programming for generating the manuscript, we
implement the following environment specification, which can be found on the
GitHub repository as a file named `generate_env.R` (@lst-rix-env-manuscript):

\small

```{r}
#| lst-label: lst-rix-env-manuscript
#| lst-cap: "Environment specification for the manuscript using rix()"
#| eval: false
#| echo: true

library(rix)

rix(
  date = "2025-08-25",
  r_pkgs = c(
    "rix",
    "quarto",
    "knitr",
    "marginaleffects",
    "simhelpers",
    "ggplot2",
    "doParallel",
    "doRNG",
    "cowplot",
    "dplyr",
    "svglite",
    "rvinecopulib"
  ),
  system_pkgs = c("quarto"),
  tex_pkgs = c(
    "amsmath",
    "ninecolors",
    "apa7",
    "scalerel",
    "threeparttable",
    "threeparttablex",
    "endfloat",
    "environ",
    "multirow",
    "tcolorbox",
    "pdfcol",
    "tikzfill",
    "fontawesome5",
    "framed",
    "newtx",
    "fontaxes",
    "xstring",
    "wrapfig",
    "tabularray",
    "siunitx",
    "fvextra",
    "geometry",
    "setspace",
    "fancyvrb",
    "anyfontsize"
  ),
  ide = "rstudio",
  project_path = ".",
  overwrite = TRUE
)
```

\normalsize

Note that we have more than just the R packages specified for the simulation
scripts. This happens because we also included what is needed for the manuscript
generation, not solely for the simulation code. In Appendix B, we mention more
specifically the reasons for adding each package in `r_pkgs()` and `tex_pkgs()`.
For now, we focus more on clarifying the different arguments for the `rix()`
function.

### The Environment Generation Script

The `rix()` function[^2] constructs this specification through a series of
parameters that collectively describe the computational environment. Each
parameter serves a distinct purpose in defining the environment's
characteristics.

[^2]: For an overarching information on the function `rix()`, we suggest the
    following {rix} documentation:
    <https://docs.ropensci.org/rix/articles/c-using-rix-to-build-project-specific-environments.html>

#### Specifying the R version

Researchers must first determine which version of R to use. This can be
accomplished in two ways: The `r_ver` argument accepts an exact version string
(e.g., "4.3.3") or special designations such as "latest-upstream" for the most
recent stable release. Alternatively, the `date` argument specifies a particular
date (e.g., "2024-11-15"), which ensures that R and all packages correspond to
the versions available on that date. The date-based approach is generally
preferable for reproducibility, as it captures a complete snapshot of the R
ecosystem at a single point in time. For this tutorial, as shown on top, we use
the `date` parameter to ensure temporal consistency across all software
components [@rodrigues_baumann_2025] (see {rix} documentation for more:
<https://docs.ropensci.org/rix/articles/d2-installing-system-tools-and-texlive-packages-in-a-nix-environment.html>).

#### Declaring R package dependencies

The `r_pkgs` argument accepts a character vector listing all required R packages
by their CRAN names. These packages will be installed from the version
repository corresponding to the specified date or R version. It is important to
list all packages that the analysis will load directly; dependencies of these
packages are automatically resolved by Nix. For packages requiring specific
versions not corresponding to the chosen date, researchers can specify exact
versions using the syntax `"packagename@version"` (e.g., `"ggplot2@2.2.1"`). For
packages available only on GitHub or other Git repositories, the `git_pkgs`
argument accepts a list structure containing repository URLs and specific commit
hashes. For example:

\small

```{r}
#| lst-label: lst-git-pkgs-example
#| lst-cap: "Example for the git_pkgs argument"
#| eval: false
#| echo: true

git_pkgs = list(
  package_name = "marginaleffects",
  repo_url = "https://github.com/vincentarelbundock/marginaleffects",
  commit = "304bff91dc31ae28b227a8485bfa4f7bdc86d625"
)
```

\normalsize

This ensures that exact development versions are obtained
[@rodrigues_baumann_2025]. For our simulation study, all packages were used with
their CRAN versions (see {rix} documentation for more details:
<https://docs.ropensci.org/rix/articles/d2-installing-system-tools-and-texlive-packages-in-a-nix-environment.html>).

#### Including system-level dependencies

Many R-based workflows require tools beyond R packages. The `system_pkgs`
parameter specifies system-level software such as Quarto for document
generation, Git for version control, or Pandoc for document conversion.
Critically, we include Quarto as a system package because this tutorial
demonstrates full computational reproducibility—not merely of the simulation
code, but of the complete manuscript itself. Our manuscript uses the `apaquarto`
extension for APA formatting, stored in the project's `_extensions/` directory
[@rodrigues_baumann_2025] (see {rix} documentation for more:
<https://docs.ropensci.org/rix/articles/d2-installing-system-tools-and-texlive-packages-in-a-nix-environment.html>).

#### Specifying LaTeX packages

The `tex_pkgs` parameter specifies LaTeX packages needed for PDF compilation.
When any packages are listed, Nix automatically includes a minimal TexLive
distribution (`scheme-small`) as a base, to which the specified packages are
added. Determining the required LaTeX packages typically involves some trial and
error—Quarto's error messages during, for example, the PDF rendering indicate
which packages are missing, and these can then be added to `tex_pkgs` (see {rix}
documentation for more:
<https://docs.ropensci.org/rix/articles/d2-installing-system-tools-and-texlive-packages-in-a-nix-environment.html>).

#### Configuring the development environment

The `ide` parameter controls whether an integrated development environment (IDE)
is included in the Nix environment, allowing users to interactively develop and
run code within their editor of choice. When `ide` is specified, the project can
be opened directly in the corresponding IDE, with all dependencies provided by
the Nix environment. For example, setting `ide = "rstudio"` installs a
project-specific version of RStudio inside the Nix environment. This is required
for RStudio because, unlike most other editors, it cannot attach to an external
Nix shell unless it is itself installed via Nix. On macOS, RStudio is only
available through Nix for R versions 4.4.3 or later (or environments dated
2025-02-28 or later); for earlier R versions, alternative editors must be used.
Other supported IDEs include Positron (`ide = "positron"`), Visual Studio Code
(`ide = "code"`), and command-line interfaces such as Radian (`ide = "radian"`).
These tools may either be installed directly within the Nix environment using
the `ide` parameter, or users may rely on an existing system installation by
setting `ide = "none"` (or `ide = "other"`) and configuring `direnv` to
automatically load the Nix environment when the project directory is opened. All
IDEs installed via Nix are project-specific and do not interfere with
system-wide installations. Detailed configuration instructions are provided in
the {rix} documentation:
<https://docs.ropensci.org/rix/articles/e-configuring-ide.html>

#### Setting file output parameters

The `project_path` parameter indicates where the `default.nix` file should be
written ("." denotes the current directory), while `overwrite` controls whether
an existing file should be replaced. Adding to this, setting `print = TRUE`,
which is another argument, displays the generated specification in the console
for immediate verification [@rodrigues_baumann_2025].

#### Multi-language environment support

While this tutorial focuses on R, researchers working across multiple
programming languages can include Python or Julia in their environments. The
`py_conf` parameter (@lst-python-pkgs-example) accepts a list specifying a
Python version and required packages (e.g.,
`py_conf = list(py_version = "3.12", py_pkgs = c("polars","pandas"))`).
Similarly, `jl_conf` enables Julia package installation. This capability is
particularly useful, for example, for projects requiring statistical computing
in R alongside machine learning pipelines in Python or numerical optimization in
Julia [@rodrigues_baumann_2025] (see {rix} documentation for more:
<https://docs.ropensci.org/rix/articles/d1-installing-r-packages-in-a-nix-environment.html>).

```{r}
#| lst-label: lst-python-pkgs-example
#| lst-cap: "Including Python packages"
#| eval: false
#| echo: true

py_conf = list(py_version = "3.12", py_pkgs = c("polars", "pandas"))

```

## Generating the Environment Specification

After defining the computational environment by running `rix()`
(@lst-rix-env-manuscript) in your R console, a file named `default.nix` is
generated. This file serves as the complete environment specification and
contains all information required to recreate the project in a fully
reproducible manner.

## Step III: Building and Using the Reproducible Environment

Once this step is complete, build the reproducible environment by navigating to
the study directory in a terminal. You may use either the integrated Terminal in
RStudio (Tools → Terminal → New Terminal) or an external system terminal from
which you are running the Nix project. From the study directory, run the
following command (Listing \@lst-nix-build):

::: {#lst-nix-build}
\small

``` bash
felipelfv@Felipes-MacBook-Pro Why-risk-it-when-you-can-rix-it % nix-build
```

Building the Nix environment \normalsize
:::

The expected output should look similar to (@lst-nix-build-output):

::: {#lst-nix-build-output}
\small

``` bash
unpacking 'https://github.com/rstats-on-nix/nixpkgs/archive/2025-08-25.tar.gz'
  into the Git cache...
warning: ignoring untrusted substituter...
warning: ignoring the client-specified setting...
/nix/store/qa7fq20m2f94szsnqzciwv8h4n81w43v-nix-shell
```

Expected output from `nix-build` \normalsize
:::

This command builds the environment according to the specification. The first
execution will download and install all required packages, which may take a few
minutes depending on network speed and system resources. Subsequent builds use
cached packages and complete in seconds. Upon successful completion, a path to
the constructed environment in the Nix store is printed (here,
`/nix/store/qa7fq20m2f94szsnqzciwv8h4n81w43v-nix-shell`), and a symbolic link
named `result` appears in the project directory pointing to this location.

Note that the warnings indicate that you are not configured as a trusted user,
so Nix cannot use the rstats-on-nix binary cache and will instead compile
packages from source, which is slower. To enable binary caching, install the
cachix client and configure the rstats-on-nix cache. See
<https://docs.ropensci.org/rix/articles/z-binary_cache.html> for instructions.

To activate the environment, run (@lst-nix-shell):

::: {#lst-nix-shell}
\small

``` bash
felipelfv@Felipes-MacBook-Pro Why-risk-it-when-you-can-rix-it % nix-shell
```

Activating the Nix environment \normalsize
:::

The expected output (if you have configured yourself as a trusted user,
otherwise the same warnings will appear) should look similar to
(@lst-nix-shell-output):

::: {#lst-nix-shell-output}
``` bash
unpacking 'https://flakehub.com/f/DeterminateSystems/nixpkgs-weekly/...' 
  into the Git cache...
[nix-shell:~/Desktop/AMPPS/Why-risk-it-when-you-can-rix-it]$
```

Expected output from `nix-shell`
:::

This command drops the user into a shell where all specified packages and tools
are available. The shell prompt changes to indicate that a Nix environment is
active (here, `[nix-shell:~/Desktop/AMPPS/Why-risk-it-when-you-can-rix-it]$`).
To verify that R is being provided by Nix rather than a system installation, run
`which R`. This should return a path within `/nix/store/`. Moreover, from within
the Nix shell, users can launch their IDE by typing its name (e.g., `rstudio` or
`positron`), which opens the IDE with the Nix environment active (@lst-shell-rs)

::: {#lst-shell-rs}
\small

``` bash
felipelfv@Felipes-MacBook-Pro Why-risk-it-when-you-can-rix-it % rstudio
```

Activating RStudio \normalsize
:::

## Reproducing the Complete Manuscript[^3]

[^3]: See the {rix} documentation for more:
    <https://docs.ropensci.org/rix/articles/z-advanced-topic-building-an-environment-for-literate-programming.html>

The manuscript source file in the project repository (`article.qmd`) combines
narrative text, executable code chunks, and references to simulation outputs. In
our specific case, we integrate the simulation run itself in the manuscript as
well as the reporting (i.e., performance metrics calculation and visualization).
In other words, the code in the separate .R files are included as code chunks
(see `article.qmd` in the GitHub repository). This is not needed and most
researchers would, in fact, probably not include the simulation run itself in
the manuscript as many simulations take days to be completed.

To render the manuscript enter the below code in the terminal
(@lst-quarto-render):

::: {#lst-quarto-render}
\small

``` bash
[nix-shell:~/Desktop/AMPPS/Why-risk-it-when-you-can-rix-it]$ 
  quarto render Manuscript/article.qmd
```

Rendering the manuscript with Quarto \normalsize
:::

This command executes all code chunks in the manuscript, incorporates results
and figures, and generates a formatted PDF following APA style guidelines via
the apaquarto extension [@schneider_2024]. This extension is saved in the
project repo already.

To download this extension for your own work you can install the extension by
using the terminal (@lst-apaquarto-install):

::: {#lst-apaquarto-install}
\small

``` bash
quarto use template wjschne/apaquarto
```

Installing the apaquarto extension \normalsize
:::

or in the console (@lst-apaquarto-r):

\small

```{r}
#| lst-label: lst-apaquarto-r
#| lst-cap: "Installing the apaquarto extension from R"
#| eval: false
#| echo: true

quarto::quarto_use_template("wjschne/apaquarto")
```

\normalsize

The final document (.docx, .pdf, or .html) is saved directly in the project
folder[^4]. Because Quarto is installed as a system-level package in our Nix
specification, the rendering occurs entirely within a fully reproducible
environment, ensuring consistent output across machines regardless of local
software configurations. If desired, the manuscript can also be reproduced
interactively by opening the project folder in the user’s preferred IDE.

[^4]: 6

As mentioned, it is worth noting that Nix shells do not fully isolate you from
your existing system by default. For R users, this has a practical implication:
packages installed in your regular R library (outside of Nix) could potentially
be loaded when running R from within the Nix environment. The {rix} package
addresses this automatically—when you call `rix()`, it also executes
`rix_init()`, which creates a project-specific `.Rprofile`. This file configures
R to ignore external package libraries and also disables `install.packages()`
within the environment. The rationale is straightforward: any new packages
should be added to `default.nix` and the environment rebuilt, preserving full
reproducibility [@rodrigues_baumann_2025]. However, for stricter isolation[^5]
that also prevents access to other system programs not specified in
`default.nix`, use the `--pure` flag (@lst-nix-shell-pure):

[^5]: For example, when preparing this manuscript without the `--pure` flag,
    `quarto render` worked successfully. However, when using the `--pure` flag,
    the build failed. Running `quarto check` from within the Nix shell (i.e.,
    `nix-shell --run "quarto check"`) revealed that Quarto was still accessing
    the system's LaTeX installation (`/Library/TeX/texbin`) rather than being
    restricted to only what was specified in `default.nix`.

::: {#lst-nix-shell-pure}
\small

``` bash
nix-shell --pure
```

Activating the Nix environment with strict isolation \normalsize
:::

```{r}
#| lst-label: lst-data-generation
#| lst-cap: "Data Generation Function"
#| echo: false
#| message: false

# fixed DGP parameters
alpha0 <- 0
alpha1 <- 0.5
alpha2 <- 0.2
beta0 <- -0.5
beta1 <- 0.7
gamma1 <- -0.4

# this will generate one dataset
gen_data <- function(n, gamma2) {
  # complete overhead. This was done just to use a package written in C++:
  # generate independent uniform pairs via independence copula, then transform to standard normal
  u <- rbicop(n, "indep", 0, numeric(0))
  X2 <- qnorm(u[, 1])
  error_X1 <- qnorm(u[, 2])

  X1 <- alpha0 + alpha1 * X2 + alpha2 * X2^2 + error_X1
  eta_true <- beta0 + beta1 * X1 + gamma1 * X2 + gamma2 * X2^2
  p <- plogis(eta_true)
  y <- rbinom(n, size = 1, prob = p)

  data.frame(y = y, x1 = X1, x2 = X2)
}
```

```{r}
#| lst-label: lst-model-estimation
#| lst-cap: "Model Estimation Functions"
#| echo: false
#| message: false

# for running one simulation replicate with MISSPECIFIED model
run_one_rep <- function(n, gamma2) {
  dat <- gen_data(n, gamma2)

  fit_misspec <- glm(y ~ x1 + x2, data = dat, family = binomial())

  ace_est <- avg_slopes(fit_misspec, variables = "x1")

  data.frame(
    est = ace_est$estimate[1],
    se = ace_est$std.error[1],
    lo = ace_est$conf.low[1],
    hi = ace_est$conf.high[1]
  )
}

# for computing the true ACE using the correctly specified model
compute_true_ace <- function(gamma2_val, seed = 12345) {
  set.seed(seed)
  bigdat <- gen_data(2e5, gamma2_val)

  fit_correct <- glm(y ~ x1 + x2 + I(x2^2), data = bigdat, family = binomial())

  avg_slopes(fit_correct, variables = "x1")$estimate[1]
}
```

```{r}
#| lst-label: lst-main-simulation-loop
#| lst-cap: "Main Simulation"
#| echo: false
#| message: false
#| output: false

# simulation parameters
nsim <- 100
ncore <- 1
master_seed <- 12345

# design matrix
n_values <- c(50, 100, 2000)
gamma2_values <- c(0, 0.3, 0.8)
gamma2_labels <- c("none", "mild", "severe")

designs <- expand.grid(
  n = n_values,
  gamma2 = gamma2_values,
  stringsAsFactors = FALSE
)

designs$confound_label <- factor(
  designs$gamma2,
  levels = gamma2_values,
  labels = gamma2_labels
)

# step 1: compute true values based on a large sample
# note that in our scenario this is possible to be done analytically
cat("Computing true ACE values\n")
true_ACE_values <- sapply(gamma2_values, compute_true_ace)
names(true_ACE_values) <- as.character(gamma2_values)
print(true_ACE_values)

# step 2: parallel processing
cl <- makeCluster(ncore)
registerDoParallel(cl)

# step 3: run simulation for all conditions
all_results <- vector("list", nrow(designs))

for (k in seq_len(nrow(designs))) {
  n_k <- designs$n[k]
  gamma2_k <- designs$gamma2[k]
  label_k <- as.character(designs$confound_label[k])

  cat("\nCondition:", label_k, "(gamma2 =", gamma2_k, "), n =", n_k, "\n")

  true_ACE_k <- true_ACE_values[as.character(gamma2_k)]

  # replications
  res_list <- foreach(
    i = seq_len(nsim),
    .packages = c("marginaleffects", "rvinecopulib")
  ) %dorng%
    {
      run_one_rep(n_k, gamma2_k)
    }

  res <- do.call(rbind, res_list)
  res$true_param <- true_ACE_k
  res$n <- n_k
  res$gamma2 <- gamma2_k
  res$confound_label <- label_k

  all_results[[k]] <- res
}

stopCluster(cl)

# for storing results
sim_results <- do.call(rbind, all_results)
```

```{r}
#| lst-label: lst-performance-metrics
#| lst-cap: "Performance Metrics"
#| echo: false
#| message: false

# unique conditions
conditions <- unique(sim_results[, c("n", "gamma2", "confound_label")])

# calculate metrics for each condition
summary_list <- vector("list", nrow(conditions))

for (k in seq_len(nrow(conditions))) {
  res <- subset(
    sim_results,
    n == conditions$n[k] &
      gamma2 == conditions$gamma2[k]
  )

  perf_abs <- calc_absolute(
    data = res,
    estimates = est,
    true_param = true_param,
    criteria = c("bias", "variance", "mse", "rmse")
  )

  perf_rel <- calc_relative(
    data = res,
    estimates = est,
    true_param = true_param,
    criteria = c("relative bias", "relative rmse")
  )

  perf_cov <- calc_coverage(
    data = res,
    lower_bound = lo,
    upper_bound = hi,
    true_param = true_param,
    criteria = c("coverage", "width")
  )

  summary_list[[k]] <- cbind(
    conditions[k, ],
    true_ace = unique(res$true_param),
    perf_abs,
    perf_rel,
    perf_cov
  )
}

performance_summary <- do.call(rbind, summary_list)
```

```{r}
#| lst-label: lst-fig-performance
#| lst-cap: "Code for generating the performance metrics visualization"
#| label: fig-performance
#| fig-cap: "Performance of ACE estimator across sample sizes and confounding severity. Panel A shows relative bias, Panel B shows relative RMSE, Panel C shows coverage probability of 95% confidence intervals (dashed line at nominal 0.95 level), and Panel D shows average confidence interval width. Results demonstrate that model misspecification induces systematic bias that persists across sample sizes, while increasing sample size improves precision but not accuracy under misspecification."
#| fig-width: 10
#| fig-height: 8
#| echo: false
#| warning: false

# Absolute bias plot
p_bias <- ggplot(
  performance_summary,
  aes(x = n, y = bias, color = confound_label, group = confound_label)
) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  geom_point(size = 3) +
  geom_line() +
  labs(
    x = "Sample size (n)",
    y = "Bias in ACE estimation",
    color = "Confounding\nnon-linearity",
    title = "Absolute Bias from Omitting X2²"
  ) +
  theme_minimal() +
  theme(legend.position = "right")

# Relative bias plot
p_rel_bias <- ggplot(
  performance_summary,
  aes(x = n, y = rel_bias, color = confound_label, group = confound_label)
) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray50") +
  geom_point(size = 3) +
  geom_line() +
  labs(
    x = "Sample size (n)",
    y = "Relative Bias",
    color = "Confounding\nnon-linearity",
    title = "Relative Bias from Omitting X2²"
  ) +
  theme_minimal() +
  theme(legend.position = "right")

# Absolute RMSE plot
p_rmse <- ggplot(
  performance_summary,
  aes(x = n, y = rmse, color = confound_label, group = confound_label)
) +
  geom_point(size = 3) +
  geom_line() +
  labs(
    x = "Sample size (n)",
    y = "RMSE of ACE estimator",
    color = "Confounding\nnon-linearity",
    title = "Absolute RMSE of ACE Estimation"
  ) +
  theme_minimal() +
  theme(legend.position = "right")

# Relative RMSE plot
p_rel_rmse <- ggplot(
  performance_summary,
  aes(x = n, y = rel_rmse, color = confound_label, group = confound_label)
) +
  geom_point(size = 3) +
  geom_line() +
  labs(
    x = "Sample size (n)",
    y = "Relative RMSE",
    color = "Confounding\nnon-linearity",
    title = "Relative Accuracy of ACE Estimation"
  ) +
  theme_minimal() +
  theme(legend.position = "right")

# Coverage plot
p_coverage <- ggplot(
  performance_summary,
  aes(x = n, y = coverage, color = confound_label, group = confound_label)
) +
  geom_hline(yintercept = 0.95, linetype = "dashed", color = "gray50") +
  geom_point(size = 3) +
  geom_line() +
  labs(
    x = "Sample size (n)",
    y = "Coverage of 95% CI",
    color = "Confounding\nnon-linearity",
    title = "CI Coverage Under Residual Confounding"
  ) +
  coord_cartesian(ylim = c(0, 1)) +
  theme_minimal() +
  theme(legend.position = "right")

# CI Width plot
p_width <- ggplot(
  performance_summary,
  aes(x = n, y = width, color = confound_label, group = confound_label)
) +
  geom_point(size = 3) +
  geom_line() +
  labs(
    x = "Sample size (n)",
    y = "Average CI width",
    color = "Confounding\nnon-linearity",
    title = "Precision of Confidence Intervals"
  ) +
  theme_minimal() +
  theme(legend.position = "right")

# the 2x2 grid with all plots
legend <- get_legend(
  p_rel_bias + theme(legend.position = "right")
)

plot_grid(
  plot_grid(p_rel_bias, p_rel_rmse, p_coverage, p_width, ncol = 2, nrow = 2),
  legend,
  rel_widths = c(3, 0.4)
)
```

```{r}
#| lst-label: lst-tbl-results
#| lst-cap: "Code for generating the performance metrics table"
#| label: tbl-results
#| tbl-cap: "Performance metrics for ACE estimator across simulation conditions"
#| echo: false

results_display <- performance_summary %>%
  select(n, confound_label, rel_bias, rel_rmse, coverage, width) %>%
  mutate(
    `Sample Size` = n,
    `Confounding` = confound_label,
    `Relative Bias` = sprintf("%.3f", rel_bias),
    `Relative RMSE` = sprintf("%.3f", rel_rmse),
    `Coverage` = sprintf("%.3f", coverage),
    `CI Width` = sprintf("%.3f", width)
  ) %>%
  select(
    `Sample Size`,
    `Confounding`,
    `Relative Bias`,
    `Relative RMSE`,
    `Coverage`,
    `CI Width`
  )

kable(results_display, align = "lccccc", row.names = FALSE)
```

### Reproducing the Simulation and Results

As previously mentioned, and although we recommend it, researchers may not want 
or know how to use dynamic document generation. In this case, one could still 
follow the same steps shown thus far focusing only on the .R files while still 
benefiting from a reproducible computational environment. 

To illustrate it, after following Steps I-III, without needing to define packages
for the document rendering in Step I, the simulation study may be 
reproduced (@lst-run-simulation) as follows:

::: {#lst-run-simulation}
\small

``` bash
[nix-shell:~/Desktop/AMPPS/Why-risk-it-when-you-can-rix-it]$ 
  Rscript Simulation_Scripts/03_run_simulation.R
```

Running the complete simulation workflow \normalsize
:::

In this same way one may also execute the .R file associated with the 
performance criteria calculation and visualization. Therefore, the key advantage 
of executing within `nix-shell` is that all dependencies—R version, 
packages, and system tools—match exactly those specified in `default.nix`.

Alternatively, individual scripts can be executed separately (i.e.,
`Rscript 03_run_simulation.R)`). Therefore, the key advantage of executing
within `nix-shell` is that all dependencies—R version, packages, and system
tools—match exactly those specified in `default.nix`.

## Additional Considerations for Advanced Workflows

### Workflow Orchestration: {targets} and {rixpress}

Complex simulation studies often benefit from workflow management systems that
track dependencies between computational steps, cache intermediate results, and
enable selective re-execution when inputs change. Two complementary approaches
exist within the Nix ecosystem: using {targets} inside a Nix environment, or
using {rixpress} to leverage Nix itself as the build automation tool.

#### Using {targets} Within Nix

As mentioned, the {targets} package [@landau_2021] provides workflow
orchestration for R-based projects. This combination ensures both computational
reproducibility (via Nix controlling the environment) and computational
efficiency (via targets' intelligent caching). To integrate {targets} with Nix,
simply include "targets" in the `r_pkgs` parameter of `rix()`, and execute the
pipeline within `nix-shell` using `Rscript -e 'targets::tar_make()'`. The
{targets} metadata directory (`_targets/`) should be excluded from version
control while the `_targets.R` configuration file should be committed alongside
`default.nix` [@rodrigues_baumann_2025]. This approach is ideal for projects
that remain within the R ecosystem and do not require different computational
environments for different pipeline steps (see {rix} documentation:
<https://docs.ropensci.org/rix/articles/z-advanced-topic-reproducible-analytical-pipelines-with-nix.html>).

#### Using {rixpress} for Polyglot Pipelines

The {rixpress} package [@rixpress], a sister package to {rix}, uses Nix itself
as the build automation tool rather than operating within a Nix environment.
Each pipeline step becomes a Nix derivation, providing hermetic builds with
sandboxed execution and content-addressable caching. The key advantage of
{rixpress} emerges in multi-language workflows: different steps can execute in
different Nix-defined environments (e.g., one step using R 4.2.0 with specific
packages, another using Python 3.12 with machine learning libraries, another
using Julia for numerical optimization). The package interface, inspired by
{targets}, uses functions like `rxp_r()`, `rxp_py()`, and `rxp_jl()` to define
pipeline steps, with automatic serialization handling data transfer between
languages. Objects are stored in the Nix store and can be inspected
interactively using helper functions like `rxp_read()` and `rxp_load()` (see
{rixpress} documentation:
<https://docs.ropensci.org/rixpress/articles/intro-concepts.html>).

## Converting Existing {renv} Projects

Many researchers have existing projects using {renv} for package management. The
`renv2nix()` function facilitates migration by reading an `renv.lock` file and
generating an equivalent Nix specification. This conversion is particularly
valuable for projects where {renv} encountered system dependency issues or where
stricter reproducibility guarantees are desired. However, researchers should
note that while {renv} snapshots R package versions, Nix additionally pins
system libraries and compilers, potentially exposing previously hidden
dependencies on system configuration [@rodrigues_baumann_2025] (see {rix}
documentation: <https://docs.ropensci.org/rix/articles/f-renv2nix.html>).

## Containerization with Docker

Institutions with existing Docker-based infrastructure may wish to combine Nix
with containers. While this might seem redundant—both technologies provide
isolation—the combination offers complementary benefits: Nix ensures
bit-reproducible builds across systems, while Docker provides a familiar
deployment mechanism for non-Nix-aware computing environments. The approach is
to use Nix as the base layer within a Docker container
[@rodrigues_baumann_2025]. This strategy is particularly relevant, for example,
for projects requiring deployment to cloud computing platforms or
high-performance computing clusters where Docker is the standard
containerization technology (see {rix} documentation:
<https://docs.ropensci.org/rix/articles/z-advanced-topic-using-nix-inside-docker.html>).

# Discussion

Reproducibility in computational research is often treated as a matter of
transparency—making data and code available. This tutorial has argued that
transparency alone is insufficient without the ability to reliably reconstruct
the computational environments in which analyses are executed. For simulation
studies in particular, where results depend critically on software versions,
system libraries, and random number generation, environment-level
reproducibility is not optional but essential.

By introducing Nix and the {rix} package, we demonstrated a practical and
accessible approach to fully specifying and rebuilding computational
environments for simulation-based research. This approach enables analyses and
manuscripts to be rerun identically across machines and over time, transforming
reproducibility from an aspirational goal into a verifiable property of the
research workflow.

Importantly, adopting environment reproducibility does not require abandoning
existing analytic practices. Nix is agnostic to programming language, editor,
workflow structure, and manuscript template, allowing researchers to retain
familiar tools while strengthening the reliability of their work. In this sense,
reproducible environments serve as enabling infrastructure—supporting, rather
than replacing, other best practices such as version control, workflow
orchestration, and transparent reporting.

If reproducibility is to function as a cornerstone of cumulative science, then
the ability to reconstruct computational environments must become a routine part
of methodological practice. Tools such as Nix and {rix} lower the barrier to
achieving this goal, making fully reproducible simulation research feasible
without requiring deep systems expertise. We hope this tutorial helps normalize
environment-level reproducibility as a standard component of rigorous
computational research in psychology and beyond.

{{< pagebreak >}}

# References

::: {#refs}
:::

{{< include appendix_a.qmd >}}

{{< include appendix_b.qmd >}}