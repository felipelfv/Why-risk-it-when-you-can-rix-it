---
title: "Why Risk it, When You Can {rix} it: A Tutorial for Computational Reproducibility Focused on Simulation Studies"
shorttitle: "Reproducibility with rix"
author:
  - name: Felipe Fontana Vieira
    corresponding: true
    email: felipe.fontanavieira@ugent.be
    affiliations:
      - name: Ghent University 
        department: Department of Data Analysis 
  - name: Jason Geller
    affiliations:
      - name: Boston College
        department: Department of Psychology and Neuroscience
  - name: Bruno Rodrigues
    affiliations:
      - name: Ministry of Research and Higher Education, Luxembourg
        department: Statistics and Data Strategy Departments
abstract: |
  Reproducibility remains limited in psychology, in part because reproducibility
  exists on a spectrum -- from sharing isolated code fragments to providing
  fully executable pipelines that ensure identical results. This article
  introduces Nix and the {rix} R package as a way to provide a comprehensive
  solution for achieving full computational reproducibility in simulation
  studies. Building on this, we also demonstrate a tutorial on how to use {rix}
  to obtain a reproducible manuscript using the apaquarto extension.

keywords: [reproducibility, Nix, simulation studies, R, computational methods]
word-count: true
authornote: ~
format:
  apaquarto-pdf:
    document-mode: man
    keep-tex: true
    include-in-header:
      text: |
        \usepackage{setspace}
        \usepackage{etoolbox}
        \raggedbottom
        % Single-space code blocks
        \AtBeginEnvironment{Shaded}{\singlespacing}
        \AtBeginEnvironment{verbatim}{\singlespacing}
  apaquarto-html: default

bibliography: references.bib
---

```{r}
#| label: load-packages
#| echo: false
#| message: false

# required packages
library(marginaleffects); library(simhelpers); library(rvinecopulib)
library(doParallel); library(doRNG); library(ggplot2); library(cowplot)
library(knitr); library(dplyr)

```

Psychological science is in the midst of a credibility revolution, which has
prompted substantial progress in how research is conducted and evaluated
[@vazire2018]. Yet, despite notable progress, a key cornerstone of science,
reproducibility (i.e., the ability to precisely reproduce the results of a study
or studies based on provided data, code, materials, and software/hardware)
remains limited [@hardwicke2020]. As a result, ensuring reproducibility remains
an open and pressing challenge for psychological science.

Addressing this gap is complicated by the fact that reproducibility is 
not a binary feature but instead exists along a continuum [@peng_2011]. 
At the lower end, researchers may share only their manuscript. Further along 
the spectrum, they might provide partial code, full analysis scripts, or 
publicly accessible datasets. At the highest level, researchers document a 
fully specified computational environment that allows others to recreate 
identical results—from raw data to final manuscript output—with minimal 
friction. Open-science initiatives have made considerable progress along 
this continuum. Journal incentives such as 
open-science badges [@kidwell_etall_2016], together with platforms like GitHub 
and the Open Science Framework, have made data and code sharing increasingly 
routine [@levenstein_lyle_2018]. However, these efforts largely occupy the 
lower and middle portions of the spectrum, focusing on *what* is shared rather 
than *how* shared materials can be executed in practice. Data and code are never 
self-sufficient; they depend on a hierarchy of software components known 
collectively as dependencies, including the version of the programming language, 
the set of packages used by the analysis, and the system libraries that those 
packages require in order to function correctly. When these dependencies differ 
from those used in the original analysis, code may fail, behave differently 
across machines, or yield conflicting numerical results 
[@baker_etall_2024; @hodges_etall_2023; @glatard_etall_2015; @nosek_etall_2022].

These issues are particularly acute for simulation studies, which rely on 
complex codebases, versioned dependencies, and intricate software configurations
[@luijken_etall_2024; @siepe_etall_2024]. To make this concrete, we use 
*computational environment* to refer to the complete
software context required for an analysis to run successfully—the programming
language version, package versions, system libraries, and operating system
[@rodrigues_2023; @rodrigues_baumann_2026_polyglot]. We define *computational
environment reproducibility* as the ability to reconstruct this entire software
stack on any machine and at any future time, such that executing the same code
yields the same numerical results. Empirical assessments show that current 
practice falls short of this ideal.
@siepe_etall_2024 report that nearly two-thirds of simulation studies in
psychology provide no accompanying code, and among those that do, documentation
of the computational environment is rarely included. This gap is consequential:
simulation studies inform methodological recommendations, meaning that
insufficient reproducibility undermines confidence in those recommendations
[@luijken_etall_2024; @white_etall_2024].

Arguably, these challenges persist because researchers must 
navigate a fragmented landscape of solutions, each addressing only part of 
the problem. Package-level managers such as {renv} [@ushey_2024] and 
{groundhog} [@simonsohn_2020] stabilize R package versions but do not manage 
the R interpreter itself or the system-level libraries those packages depend on. 
Workflow orchestration tools such as {targets} [@landau_2021] and 
Make [@feldman_1979] support reproducibility in a different sense: 
they specify the structure of an analysis
by formalizing the order in which steps should run and by tracking dependencies
among intermediate results. These tools clarify *how* an analysis proceeds, but
they assume that the software stack required to run each step is already stable.
Containerization tools such as Docker, including R-focused implementations like
Rocker **\[Isnt rocker a R instance of Docker? JG\]** [@boettiger_2015;
@boettiger_eddelbuettel_2017] offer a more comprehensive approach by bundling
the full environment—operating system, system libraries, interpreter versions,
and packages—into a single executable image. Yet their use requires familiarity
with Linux system administration, and even containerization may suffer from
temporal drift when Dockerfiles rely on mutable upstream repositories
[@malka2024]. For a detailed comparison of these tools and their limitations,
see @rodrigues_baumann_2026_polyglot. Researchers thus face a difficult choice
between solutions that are accessible but incomplete or approaches that are
powerful but demand substantial technical expertise.

In this article, therefore, we focus specifically on computational environment
reproducibility as the foundation upon which other reproducibility practices
depend. For that, we introduce Nix [@dolstra_etall_2004], a functional package
manager designed to make software installation deterministic, and {rix}
[@rodrigues_baumann_2025], an R interface that allows researchers to use Nix
without needing deep knowledge of its underlying language or infrastructure. Our
objective is not to introduce a specific workflow orchestration system or to
prescribe a particular analytic structure. Instead, we aim to show how Nix and
{rix} can establish a stable, cross-platform environment within which any
analysis—whether organized in simple, documented script sequences 
(e.g., .R files that `source()` others), through more formal orchestration 
tools (e.g., targets) or embedded as code chunks in `.Rmd` or `.qmd` —can be 
executed reliably.

We illustrate these ideas through a reproducible simulation study conducted in 
R, culminating in an automated APA-formatted manuscript generated with
`apaquarto` [@schneider_2024]. Although the example centers on R because of its
prominence in psychological methodology, the principles underlying environment
reproducibility apply equally to other languages, including Python and Julia,
and to different development environments such as RStudio, VS Code, Emacs, or 
Positron. Later in the article, we briefly comment on rixpress [@rixpress], 
which extends Nix-based reproducibility to workflows spanning multiple 
languages. Throughout, our emphasis remains squarely on the reproducibility of 
computational environments as the essential
basis for transparent, reliable, and durable scientific workflows.

# A Practical Example: Setting up a Reproducible Simulation Study with {rix}

Before proceeding with the technical implementation and tutorial, we refer
readers to the Appendix A, which presents a simulation study scenario designed
to ground the subsequent discussion in a concrete example[^1]. The scenario
mimics a typical methods manuscript, describing both the statistical design and
the computational details one might encounter in practice. While the example is
illustrative rather than substantive—readers can follow the tutorial without
engaging with the specifics of the simulation—we include it to anchor the
discussion in something more tangible.

[^1]: Note that the rationale, programming-related choices, and results should
    not be used for substantial interpretation. For instance, the usage of
    {rvinecopulib} and the parallel set-up may be considered an overhead.

Simulation studies typically structure code into multiple component files, 
each handling a distinct analytical phase. We provide our implementation in 
this conventional format, organized into five 
sequential files[^2]: data generation (`01_data_generation.R`), statistical 
model specification (`02_models.R`), simulation execution 
(`03_run_simulation.R`), performance metric calculation 
(`04_performance_metrics.R`), and results visualization (`05_plots.R`). However, 
because our focus is on the reproducibility of the complete manuscript, we embed 
all simulation code directly within this manuscript as executable code chunks 
in the `.qmd` file. When this document is rendered, the simulation runs in 
its entirety (i.e., from data generation through final 
visualizations). This is not needed and, in fact, most simulation studies are 
too long to be included in the manuscript compilation. We will come back to 
this later in the tutorial. 

[^2]: Readers can find the files for these simulations here:
    <https://github.com/felipelfv/Why-risk-it-when-you-can-rix-it>

[^3]: For instance, in this context, we use parallel processing which requires
    specifying the amount of cores to be used. This type of information should
    be mentioned.

Notably, the workflow depends on several R packages (see Appendix A)[^3]. 
Thus, let's pretend a researcher whats to reproduce our simulation and results 
reported in the article. 
What might prevent them from obtaining identical results? The most
immediate concern may involve package versions. A researcher installing packages
today may encounter errors if functions have been renamed or deprecated, or may
obtain results that differ subtly due to changes in computational defaults.
Beyond package versions, some R packages depend on system-level libraries that
must be installed separately from R itself. Our simulation illustrates this
directly: the {rvinecopulib} package provides an interface to a C++ library and
links against Boost, Eigen, and RcppThread [@rvinecopulib]. Moreover, the R
language itself introduces version dependencies. Code written with R 4.3 may use
syntax or functions unavailable in R 4.0 **\[\*\* % vs. \|\>?JG\*\*\]**. More
subtly, R's random number generator has changed across major versions, meaning
that identical code with identical seeds can produce different random sequences
depending on the R version. For simulation studies where reproducibility of
specific random draws matters, this version sensitivity is consequential.
Moreover, if one also employs literate programming for the manuscript as in our 
case, additional dependencies arise: R Markdown or Quarto documents require 
pandoc, and PDF output requires a LaTeX distribution—each with its 
own versioning and platform-specific installation.

# Nix and {rix}: A Comprehensive Solution

A potential solution to the above issue is Nix [@dolstra_etall_2004]. Nix is a
package manager \[**Bruno you once used the analogy of Nix like the apple
application store. I kind of like this because it is more than a package manger.
(JG)**\] built around declarative, isolated environments rather than ad hoc
system-level installations. Unlike familiar tools such as `install.packages()`
in R or `apt-get` on Linux, Nix manages language versions, package versions,
system dependencies, and cross-platform consistency through a single framework
[@rodrigues_baumann_2025]. Rather than modifying shared directories, Nix builds
each environment as an explicit, self-contained specification. This addresses
the fragmented landscape described before: where researchers currently must
coordinate separate tools for package management, interpreter versions, and
system dependencies, Nix handles all three within a unified declarative model.

## Core Principles

Rather than installing software into global directories (e.g., `/usr/lib`), Nix
places every package in its own directory under `/nix/store`. Each package path
contains a cryptographic hash representing its precise inputs—source code,
dependencies, and build instructions. Because these paths are content-addressed,
multiple versions of the same software can coexist without conflict. A
researcher can, for example, maintain projects requiring R 4.1.0 and R 4.3.3
side by side, or use different package versions across analyses, switching
between them seamlessly [@rodrigues_baumann_2025].

The Nix ecosystem is built around nixpkgs, a version-controlled repository
comprising more than 120,000 packages, including nearly all of CRAN and
Bioconductor. By pinning a specific commit or date, researchers freeze the
entire software stack—R itself, R packages, and all system libraries—at that
point in time. This eliminates the system-dependency problems that tools like
{renv} cannot address [@rodrigues_baumann_2025].

This architecture also ensures stability over time. Empirical work has shown
strong rebuildability and reproducibility rates for historical nixpkgs snapshots
[@rodrigues_baumann_2026_polyglot]. Combined with binary caches, which often
allow environments to materialize in seconds, Nix becomes practical for
interactive research workflows [@rodrigues_baumann_2025].

## The {rix} Package: R Interface to Nix

Nix expressions are written in a dedicated functional language unfamiliar to
most researchers. The {rix} package removes this barrier by providing an
R-native interface. A single call to `rix()` generates complete Nix
configurations from standard R syntax, specifying R versions, CRAN packages,
system libraries, and even Python or Julia components when required. Users never
need to read or write Nix code directly, as {rix} performs the translation
automatically [@rodrigues_baumann_2025].

A key feature of {rix} is its integration with rstats-on-nix, a
community-maintained fork offering daily CRAN snapshots and weekly tested
environments on Linux and macOS. Researchers can request, for example,
`rix(date = "2024-12-14")` to obtain a validated and reproducible environment
without manually assessing compatibility. After the configuration is generated,
`nix_build()` instantiates the environment, and binary caches typically allow
this to complete within seconds [@rodrigues_baumann_2025].

Although Nix is capable of replacing tools like Docker for isolation or {renv}
for package management, it does not require an all-or-nothing transition.
Researchers can adopt it gradually and use it alongside familiar tooling. For
instance, by building Docker images with Nix, converting existing {renv}
lockfiles, or running {targets} pipelines within a Nix-defined environment
[@rodrigues_baumann_2025]. This allows Nix to strengthen reproducibility while
preserving established workflows. For projects requiring more sophisticated
pipeline management, {rixpress} extends Nix’s guarantees to workflow
orchestration, enabling step-level isolation across languages, though such
capabilities lie beyond the present focus on environment reproducibility. We
will come back to this after the tutorial.

## Step I: Installing Nix and {rix}

Before proceeding, both Nix and the {rix} R package need to be installed.
Installation procedures differ across operating systems (Windows via WSL2,
Linux, and macOS), and detailed, up-to-date instructions are maintained in the
official {rix} documentation:

-   **Linux and Windows (WSL2)**:
    <https://docs.ropensci.org/rix/articles/b1-setting-up-and-using-rix-on-linux-and-windows.html>
-   **macOS**:
    <https://docs.ropensci.org/rix/articles/b2-setting-up-and-using-rix-on-macos.html>

Once Nix is installed, {rix} can be installed from CRAN (@lst-install-rix-cran), 
or directly through Nix without requiring a pre-existing R installation 
(@lst-install-rix-nix; see the documentation links above for details).
2
```{r}
#| lst-label: lst-install-rix-cran
#| lst-cap: "Installing {rix} from CRAN"
#| eval: false
#| echo: true
install.packages("rix")
```

::: {#lst-install-rix-nix}
\small
``` bash
felipelfv@Felipes-MacBook-Pro Why-risk-it-when-you-can-rix-it %
  nix-shell --expr "$(curl -sl https://raw.githubusercontent.com/ropensci/rix/main/inst/extdata/default.nix)"
```
Installing {rix} directly via Nix (no R required) \normalsize
:::

It is worth noting that {rix} can generate Nix expressions even without Nix
installed on your system—you can write a `default.nix` file without Nix, but you
cannot build or enter the resulting environment unless Nix is installed
[@rodrigues_baumann_2025].

## Step II: Specifying the Computational Environment {#sec-specifying-environment}

The initial step in establishing a reproducible environment is to create a
script that will generate the environment specification. We recommend creating a
file named `generate_env.R` (or similar) in the project directory. This script
will use the `rix()` function from the {rix} package to produce a `default.nix`
file—a declarative specification that precisely defines all software
dependencies required for the project.

In the case where we use literate programming for generating the manuscript, we 
implement the following environment specification, which can be
found on the GitHub repository as a file named `generate_env.R`
(@lst-rix-env-manuscript):

\small

```{r}
#| lst-label: lst-rix-env-manuscript
#| lst-cap: "Environment specification for the manuscript using rix()"
#| eval: false
#| echo: true

library(rix)

rix(
  date = "2025-08-25", 
  r_pkgs = c(
    "rix", "quarto", "knitr", "marginaleffects", "simhelpers",
    "ggplot2", "doParallel", "doRNG", "cowplot", "dplyr", "svglite",
    "rvinecopulib"
  ),
  system_pkgs = c("quarto"), 
  tex_pkgs = c( 
    "amsmath", "ninecolors", "apa7", "scalerel", "threeparttable",
    "threeparttablex", "endfloat", "environ", "multirow", "tcolorbox",
    "pdfcol", "tikzfill", "fontawesome5", "framed", "newtx",
    "fontaxes", "xstring", "wrapfig", "tabularray", "siunitx",
    "fvextra", "geometry", "setspace", "fancyvrb", "anyfontsize"
  ),
  ide = "rstudio", 
  project_path = ".", 
  overwrite = TRUE 
)
```

\normalsize

Note that we have more than just the R packages specified for the simulation
scripts. This happens because we also included what is needed for the manuscript
generation, not solely for the simulation code. In Appendix B, we mention more
specifically the reasons for adding each package in `r_pkgs()` and `tex_pkgs()`.
For now, we focus more on clarifying the different arguments for the `rix()`
function.

### The Environment Generation Script

The `rix()` function[^4] constructs this specification through a series of
parameters that collectively describe the computational environment. Each
parameter serves a distinct purpose in defining the environment's
characteristics.

[^4]: For an overarching information on the function `rix()`, we suggest the
    following {rix} documentation:
    <https://docs.ropensci.org/rix/articles/c-using-rix-to-build-project-specific-environments.html>

#### Specifying the R version 

Researchers must first determine which version of R to use. This can be
accomplished in two ways: The `r_ver` argument accepts an exact version string
(e.g., "4.3.3") or special designations such as "latest-upstream" for the most
recent stable release. Alternatively, the `date` argument specifies a particular
date (e.g., "2024-11-15"), which ensures that R and all packages correspond to
the versions available on that date. The date-based approach is generally
preferable for reproducibility, as it captures a complete snapshot of the R
ecosystem at a single point in time. For this tutorial, as shown on top, we use
the `date` parameter to ensure temporal consistency across all software
components [@rodrigues_baumann_2025] (see {rix} documentation for more:
<https://docs.ropensci.org/rix/articles/d2-installing-system-tools-and-texlive-packages-in-a-nix-environment.html>).

#### Declaring R package dependencies 

The `r_pkgs` argument accepts a character vector listing all required R packages
by their CRAN names. These packages will be installed from the version
repository corresponding to the specified date or R version. It is important to
list all packages that the analysis will load directly; dependencies of these
packages are automatically resolved by Nix. For packages requiring specific
versions not corresponding to the chosen date, researchers can specify exact
versions using the syntax `"packagename@version"` (e.g., `"ggplot2@2.2.1"`). For
packages available only on GitHub or other Git repositories, the `git_pkgs`
argument accepts a list structure containing repository URLs and specific commit
hashes. For example:

\small

```{r}
#| lst-label: lst-git-pkgs-example
#| lst-cap: "Example for the git_pkgs argument"
#| eval: false
#| echo: true

git_pkgs = list(
    package_name = "marginaleffects",
    repo_url = "https://github.com/vincentarelbundock/marginaleffects",
    commit = "304bff91dc31ae28b227a8485bfa4f7bdc86d625"
)
```

\normalsize

This ensures that exact development versions are obtained
[@rodrigues_baumann_2025]. For our simulation study, all packages were used with
their CRAN versions (see {rix} documentation for more details:
<https://docs.ropensci.org/rix/articles/d2-installing-system-tools-and-texlive-packages-in-a-nix-environment.html>).

#### Including system-level dependencies 

Many R-based workflows require tools beyond R packages. The `system_pkgs`
parameter specifies system-level software such as Quarto for document
generation, Git for version control, or Pandoc for document conversion.
Critically, we include Quarto as a system package because this tutorial
demonstrates full computational reproducibility—not merely of the simulation
code, but of the complete manuscript itself. Our manuscript uses the `apaquarto`
extension for APA formatting, stored in the project's `_extensions/` directory
[@rodrigues_baumann_2025] (see {rix} documentation for more:
<https://docs.ropensci.org/rix/articles/d2-installing-system-tools-and-texlive-packages-in-a-nix-environment.html>).

#### Specifying LaTeX packages 

The `tex_pkgs` parameter specifies LaTeX packages needed for PDF compilation.
When any packages are listed, Nix automatically includes a minimal TexLive
distribution (`scheme-small`) as a base, to which the specified packages are
added. Determining the required LaTeX packages typically involves some trial and
error—Quarto's error messages during, for example, the PDF rendering indicate
which packages are missing, and these can then be added to `tex_pkgs` (see {rix}
documentation for more:
<https://docs.ropensci.org/rix/articles/d2-installing-system-tools-and-texlive-packages-in-a-nix-environment.html>).

#### Configuring the development environment 

The `ide` parameter determines whether an integrated development environment
should be included in the Nix environment. Setting `ide = "rstudio"` installs a
project-specific version of RStudio within the Nix environment. This is required
for RStudio because, unlike other editors, RStudio cannot interact with Nix
shells unless it is itself installed via Nix. Note that on macOS, RStudio is
only available through Nix for R versions 4.4.3 or later (or dates after
2025-02-28); for earlier versions, alternative editors must be used. Other
supported IDEs include Positron (`ide = "positron"`), Visual Studio Code
(`ide = "code"`), and command-line tools such as Radian (`ide = "radian"`).
These editors can either be installed within the Nix environment using the `ide`
parameter, or researchers can use an already-installed version by setting
`ide = "none"` (or `ide = "other"`) and configuring `direnv` to automatically
load the Nix environment when opening the project folder. Each IDE installed via
Nix is project-specific and will not interfere with system-wide installations.
See the {rix} documentation for detailed configuration instructions:
<https://docs.ropensci.org/rix/articles/e-configuring-ide.html>.

#### Setting file output parameters 

The `project_path` parameter indicates where the `default.nix` file should be
written ("." denotes the current directory), while `overwrite` controls whether
an existing file should be replaced. Adding to this, setting `print = TRUE`,
which is another argument, displays the generated specification in the console
for immediate verification [@rodrigues_baumann_2025].

#### Multi-language environment support

While this tutorial focuses on R, researchers working across multiple
programming languages can include Python or Julia in their environments. The
`py_conf` parameter accepts a list specifying a Python version and required
packages (e.g.,
`py_conf = list(py_version = "3.12", py_pkgs = c("polars","pandas"))`).
Similarly, `jl_conf` enables Julia package installation. This capability is
particularly useful, for example, for projects requiring statistical computing
in R alongside machine learning pipelines in Python or numerical optimization in
Julia [@rodrigues_baumann_2025] (see {rix} documentation for more:
<https://docs.ropensci.org/rix/articles/d1-installing-r-packages-in-a-nix-environment.html>).

```{r}
#| lst-label: lst-python-pkgs-example
#| lst-cap: "Including Python packages"
#| eval: false
#| echo: true

py_conf = list(py_version = "3.12", 
               py_pkgs = c("polars","pandas"))

```

\[**We should include a code chunk like the github one above showing how to add
python packages?JG**\]

### Generating the Environment Specification

To execute the .R script, either run from the
terminal `Rscript generate_env.R`, source it from within R using
`source("generate_env.R")`, or run the `rix::rix()` function above. This will
generate the `default.nix` file in the project directory. This file serves as
the formal, machine-readable specification of the computational environment.
Importantly, `rix()` automatically invokes `rix_init()`, which creates a
project-specific `.Rprofile` file that prevents package library conflicts and
disables `install.packages()` to maintain environment integrity. There may be
exemptions to this, which we comment briefly in the next section.

## Step III: Building and Using the Reproducible Environment

Once the `default.nix` file has been generated, the next step is to build the
environment and use it to reproduce the complete manuscript. 

### Building the Environment

From the terminal, navigate to the study directory containing the `default.nix`
file and execute (@lst-nix-build):

::: {#lst-nix-build}
\small

``` bash
felipelfv@Felipes-MacBook-Pro Why-risk-it-when-you-can-rix-it % nix-build
```
Building the Nix environment \normalsize
:::

The expected output should look similar to (@lst-nix-build-output):

::: {#lst-nix-build-output}
\small

``` bash
unpacking 'https://github.com/rstats-on-nix/nixpkgs/archive/2025-08-25.tar.gz'
  into the Git cache...
warning: ignoring untrusted substituter...
warning: ignoring the client-specified setting...
/nix/store/qa7fq20m2f94szsnqzciwv8h4n81w43v-nix-shell
```
Expected output from `nix-build`  \normalsize
:::

This command builds the environment according to the specification. The first
execution will download and install all required packages, which may take a few
minutes depending on network speed and system resources. Subsequent builds use
cached packages and complete in seconds. Upon successful completion, a path
to the constructed environment in the Nix store is printed (here,
`/nix/store/qa7fq20m2f94szsnqzciwv8h4n81w43v-nix-shell`), and a symbolic
link named `result` appears in the project directory pointing to this location.

Note that the warnings indicate that you are not configured as a trusted user, 
so Nix cannot use the rstats-on-nix binary cache and will instead compile 
packages from source, which is slower. To enable binary caching, 
install the cachix client and configure the rstats-on-nix cache. See 
<https://docs.ropensci.org/rix/articles/z-binary_cache.html> for instructions.

To activate the environment, run (@lst-nix-shell):

::: {#lst-nix-shell}
\small

``` bash
felipelfv@Felipes-MacBook-Pro Why-risk-it-when-you-can-rix-it % nix-shell
```

Activating the Nix environment \normalsize
:::

The expected output (if you have configured yourself as a trusted user, 
otherwise the same warnings will appear) should look similar to 
(@lst-nix-shell-output):

::: {#lst-nix-shell-output}
``` bash
unpacking 'https://flakehub.com/f/DeterminateSystems/nixpkgs-weekly/...' 
  into the Git cache...
[nix-shell:~/Desktop/AMPPS/Why-risk-it-when-you-can-rix-it]$
```
Expected output from `nix-shell`
:::

This command drops the user into a shell where all specified packages and tools
are available. The shell prompt changes to indicate that a Nix environment is 
active (here, `[nix-shell:~/Desktop/AMPPS/Why-risk-it-when-you-can-rix-it]$`). 
To verify that R is being provided by Nix rather than a system installation, 
run `which R`. This should return a path within `/nix/store/`. Moreover, from 
within the Nix shell, users can launch their IDE by typing its name 
(e.g., `rstudio` or `positron`), which opens the IDE with the Nix environment 
active.

## Reproducing the Complete Manuscript[^5]

[^5]: See the {rix} documentation for more:
    <https://docs.ropensci.org/rix/articles/z-advanced-topic-building-an-environment-for-literate-programming.html>

The manuscript source file (`article.qmd`) combines narrative text, executable 
code chunks, and references to simulation outputs. In
our specific case, we integrate the simulation run itself in the manuscript as
well as the reporting (i.e., performance metrics calculation and visualization).
In other words, the code in the separate .R files are included as code chunks
(see `article.qmd` in the GitHub repository). This is not needed and most 
researchers would, in fact, probably not include the
simulation run itself in the manuscript as many simulations take days to be
completed. 

To render the manuscript (@lst-quarto-render):

::: {#lst-quarto-render}
\small

``` bash
[nix-shell:~/Desktop/AMPPS/Why-risk-it-when-you-can-rix-it]$ 
  quarto render Manuscript/article.qmd
```

Rendering the manuscript with Quarto \normalsize
:::

This command executes all code chunks in the manuscript, incorporates results
and figures, and generates a formatted PDF following APA style guidelines via
the apaquarto extension [@schneider_2024]. This extension is saved in the
project repo already.

To download this extension for your own work you can install the extension by
using the terminal (@lst-apaquarto-install):

::: {#lst-apaquarto-install}
\small

``` bash
quarto use template wjschne/apaquarto
```

Installing the apaquarto extension \normalsize
:::

or in the console (@lst-apaquarto-r):

\small

```{r}
#| lst-label: lst-apaquarto-r
#| lst-cap: "Installing the apaquarto extension from R"
#| eval: false
#| echo: true

quarto::quarto_use_template("wjschne/apaquarto")
```

\normalsize

The final document (.docx, .pdf, or .html) is saved directly in the project
folder^[6]. Because Quarto is installed as a system-level package in our Nix
specification, the rendering occurs entirely within a fully reproducible
environment, ensuring consistent output across machines regardless of local
software configurations. If desired, the manuscript can also be reproduced
interactively by opening the project folder in the user’s preferred IDE and
running the code chunks directly.

[^6]: Although we use *apaquarto* in this example, many
    alternative manuscript templates are available, and Nix is agnostic to the
    specific template employed, provided the necessary extensions are installed.

As mentioned, it is worth noting that Nix shells do not fully isolate you from
your existing system by default. For R users, this has a practical implication:
packages installed in your regular R library (outside of Nix) could potentially
be loaded when running R from within the Nix environment. The {rix} package
addresses this automatically—when you call `rix()`, it also executes
`rix_init()`, which creates a project-specific `.Rprofile`. This file configures
R to ignore external package libraries and also disables `install.packages()`
within the environment. The rationale is straightforward: any new packages
should be added to `default.nix` and the environment rebuilt, preserving full
reproducibility [@rodrigues_baumann_2025]. However, for stricter isolation[^7]
that also prevents access to other system programs not specified in
`default.nix`, use the `--pure` flag (@lst-nix-shell-pure):

[^7]: For example, when preparing this manuscript without the `--pure` flag,
    `quarto render` worked successfully. However, when using the `--pure` flag,
    the build failed. Running `quarto check` from within the Nix shell (i.e.,
    `nix-shell --run "quarto check"`) revealed that Quarto was still accessing
    the system's LaTeX installation (`/Library/TeX/texbin`) rather than being
    restricted to only what was specified in `default.nix`.

::: {#lst-nix-shell-pure}
\small

``` bash
nix-shell --pure
```

Activating the Nix environment with strict isolation \normalsize
:::

```{r}
#| lst-label: lst-data-generation
#| lst-cap: "Data Generation Function"
#| echo: false
#| message: false

# fixed DGP parameters
alpha0 <- 0
alpha1 <- 0.5   
alpha2 <- 0.2   
beta0  <- -0.5
beta1  <- 0.7   
gamma1 <- -0.4  

# this will generate one dataset
gen_data <- function(n, gamma2) {
  
  # complete overhead. This was done just to use a package written in C++:
  # generate independent uniform pairs via independence copula, then transform to standard normal
  u <- rbicop(n, "indep", 0, numeric(0))
  X2 <- qnorm(u[, 1])
  error_X1 <- qnorm(u[, 2])
  
  X1 <- alpha0 + alpha1 * X2 + alpha2 * X2^2 + error_X1
  eta_true <- beta0 + beta1 * X1 + gamma1 * X2 + gamma2 * X2^2
  p <- plogis(eta_true)
  y <- rbinom(n, size = 1, prob = p)
  
  data.frame(y = y, x1 = X1, x2 = X2)
}
```

```{r}
#| lst-label: lst-model-estimation
#| lst-cap: "Model Estimation Functions"
#| echo: false
#| message: false

# for running one simulation replicate with MISSPECIFIED model
run_one_rep <- function(n, gamma2) {
  dat <- gen_data(n, gamma2)
  
  fit_misspec <- glm(y ~ x1 + x2, 
                     data = dat, 
                     family = binomial())
  
  ace_est <- avg_slopes(fit_misspec, variables = "x1")
  
  data.frame(
    est = ace_est$estimate[1],
    se  = ace_est$std.error[1],
    lo  = ace_est$conf.low[1],
    hi  = ace_est$conf.high[1]
  )
}

# for computing the true ACE using the correctly specified model
compute_true_ace <- function(gamma2_val, seed = 12345) {
  set.seed(seed)
  bigdat <- gen_data(2e5, gamma2_val)
  
  fit_correct <- glm(y ~ x1 + x2 + I(x2^2), 
                     data = bigdat, 
                     family = binomial())
  
  avg_slopes(fit_correct, variables = "x1")$estimate[1]
}
```

```{r}
#| lst-label: lst-main-simulation-loop
#| lst-cap: "Main Simulation"
#| echo: false
#| message: false
#| output: false

# simulation parameters
nsim  <- 100
ncore <- 1
master_seed <- 12345

# design matrix
n_values <- c(50, 100, 2000)
gamma2_values <- c(0, 0.3, 0.8)
gamma2_labels <- c("none", "mild", "severe")

designs <- expand.grid(
  n = n_values,
  gamma2 = gamma2_values,
  stringsAsFactors = FALSE
)

designs$confound_label <- factor(
  designs$gamma2,
  levels = gamma2_values,
  labels = gamma2_labels
)

# step 1: compute true values based on a large sample
# note that in our scenario this is possible to be done analytically
cat("Computing true ACE values\n")
true_ACE_values <- sapply(gamma2_values, compute_true_ace)
names(true_ACE_values) <- as.character(gamma2_values)
print(true_ACE_values)

# step 2: parallel processing
cl <- makeCluster(ncore)
registerDoParallel(cl)

# step 3: run simulation for all conditions
all_results <- vector("list", nrow(designs))

for (k in seq_len(nrow(designs))) {
  n_k      <- designs$n[k]
  gamma2_k <- designs$gamma2[k]
  label_k  <- as.character(designs$confound_label[k])
  
  cat("\nCondition:", label_k, "(gamma2 =", gamma2_k, "), n =", n_k, "\n")
  
  true_ACE_k <- true_ACE_values[as.character(gamma2_k)]
  
  # replications
  res_list <- foreach(
    i = seq_len(nsim),
    .packages = c("marginaleffects", "rvinecopulib")
  ) %dorng% {
    run_one_rep(n_k, gamma2_k)
  }
  
  res <- do.call(rbind, res_list)
  res$true_param <- true_ACE_k
  res$n <- n_k
  res$gamma2 <- gamma2_k
  res$confound_label <- label_k
  
  all_results[[k]] <- res
}

stopCluster(cl)

# for storing results
sim_results <- do.call(rbind, all_results)
```

```{r}
#| lst-label: lst-performance-metrics
#| lst-cap: "Performance Metrics"
#| echo: false
#| message: false

# unique conditions
conditions <- unique(sim_results[, c("n", "gamma2", "confound_label")])

# calculate metrics for each condition
summary_list <- vector("list", nrow(conditions))

for (k in seq_len(nrow(conditions))) {
  res <- subset(sim_results, 
                n == conditions$n[k] & 
                  gamma2 == conditions$gamma2[k])
  
  perf_abs <- calc_absolute(
    data = res,
    estimates = est,
    true_param = true_param,
    criteria = c("bias", "variance", "mse", "rmse")
  )
  
  perf_rel <- calc_relative(
    data = res,
    estimates = est,
    true_param = true_param,
    criteria = c("relative bias", "relative rmse")
  )
  
  perf_cov <- calc_coverage(
    data = res,
    lower_bound = lo,
    upper_bound = hi,
    true_param = true_param,
    criteria = c("coverage", "width")
  )
  
  summary_list[[k]] <- cbind(
    conditions[k, ],
    true_ace = unique(res$true_param),
    perf_abs,
    perf_rel,
    perf_cov
  )
}

performance_summary <- do.call(rbind, summary_list)
```

```{r}
#| lst-label: lst-fig-performance
#| lst-cap: "Code for generating the performance metrics visualization"
#| label: fig-performance
#| fig-cap: "Performance of ACE estimator across sample sizes and confounding severity. Panel A shows relative bias, Panel B shows relative RMSE, Panel C shows coverage probability of 95% confidence intervals (dashed line at nominal 0.95 level), and Panel D shows average confidence interval width. Results demonstrate that model misspecification induces systematic bias that persists across sample sizes, while increasing sample size improves precision but not accuracy under misspecification."
#| fig-width: 10
#| fig-height: 8
#| echo: false
#| warning: false

# Absolute bias plot
p_bias <- ggplot(performance_summary,
                 aes(x = n, y = bias,
                     color = confound_label, group = confound_label)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  geom_point(size = 3) +
  geom_line() +
  labs(x = "Sample size (n)",
       y = "Bias in ACE estimation",
       color = "Confounding\nnon-linearity",
       title = "Absolute Bias from Omitting X2²") +
  theme_minimal() +
  theme(legend.position = "right")

# Relative bias plot
p_rel_bias <- ggplot(performance_summary,
                     aes(x = n, y = rel_bias,
                         color = confound_label, group = confound_label)) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray50") +
  geom_point(size = 3) +
  geom_line() +
  labs(x = "Sample size (n)",
       y = "Relative Bias",
       color = "Confounding\nnon-linearity",
       title = "Relative Bias from Omitting X2²") +
  theme_minimal() +
  theme(legend.position = "right")

# Absolute RMSE plot
p_rmse <- ggplot(performance_summary,
                 aes(x = n, y = rmse,
                     color = confound_label, group = confound_label)) +
  geom_point(size = 3) +
  geom_line() +
  labs(x = "Sample size (n)",
       y = "RMSE of ACE estimator",
       color = "Confounding\nnon-linearity",
       title = "Absolute RMSE of ACE Estimation") +
  theme_minimal() +
  theme(legend.position = "right")

# Relative RMSE plot
p_rel_rmse <- ggplot(performance_summary,
                     aes(x = n, y = rel_rmse,
                         color = confound_label, group = confound_label)) +
  geom_point(size = 3) +
  geom_line() +
  labs(x = "Sample size (n)",
       y = "Relative RMSE",
       color = "Confounding\nnon-linearity",
       title = "Relative Accuracy of ACE Estimation") +
  theme_minimal() +
  theme(legend.position = "right")

# Coverage plot
p_coverage <- ggplot(performance_summary,
                     aes(x = n, y = coverage,
                         color = confound_label, group = confound_label)) +
  geom_hline(yintercept = 0.95, linetype = "dashed", color = "gray50") +
  geom_point(size = 3) +
  geom_line() +
  labs(x = "Sample size (n)",
       y = "Coverage of 95% CI",
       color = "Confounding\nnon-linearity",
       title = "CI Coverage Under Residual Confounding") +
  coord_cartesian(ylim = c(0, 1)) +
  theme_minimal() +
  theme(legend.position = "right")

# CI Width plot
p_width <- ggplot(performance_summary,
                  aes(x = n, y = width,
                      color = confound_label, group = confound_label)) +
  geom_point(size = 3) +
  geom_line() +
  labs(x = "Sample size (n)",
       y = "Average CI width",
       color = "Confounding\nnon-linearity",
       title = "Precision of Confidence Intervals") +
  theme_minimal() +
  theme(legend.position = "right")

# the 2x2 grid with all plots
legend <- get_legend(
  p_rel_bias + theme(legend.position = "right")
)

plot_grid(
  plot_grid(p_rel_bias, p_rel_rmse, p_coverage, p_width, 
            ncol = 2, nrow = 2),
  legend,
  rel_widths = c(3, 0.4)
)
```

```{r}
#| lst-label: lst-tbl-results
#| lst-cap: "Code for generating the performance metrics table"
#| label: tbl-results
#| tbl-cap: "Performance metrics for ACE estimator across simulation conditions"
#| echo: false

results_display <- performance_summary %>%
  select(n, confound_label, rel_bias, rel_rmse, coverage, width) %>%
  mutate(
    `Sample Size` = n,
    `Confounding` = confound_label,
    `Relative Bias` = sprintf("%.3f", rel_bias),
    `Relative RMSE` = sprintf("%.3f", rel_rmse),
    `Coverage` = sprintf("%.3f", coverage),
    `CI Width` = sprintf("%.3f", width)
  ) %>%
  select(`Sample Size`, `Confounding`, `Relative Bias`, 
         `Relative RMSE`, `Coverage`, `CI Width`)

kable(results_display, align = "lccccc", row.names = FALSE)
```

### Reproducing the Simulation

As described before, [...] For streamlined
execution, we provide a master script (`06_run_all.R`) that runs the complete
simulation and calculates the results as well as visualizations. The script
begins by loading the required packages, then proceeds through three steps:
first, it sources `03_run_simulation.R`, which in turn loads the data generation
function (`01_data_generation.R`) and the model estimation functions
(`02_models.R`) to run the Monte Carlo simulation; second, it sources
`04_performance_metrics.R` to calculate the performance criteria from the saved
simulation results; and third, it sources `05_plots.R` to generate the figures
(see @fig-performance). The script has the following content (@lst-run-all):

\small

```{r}
#| lst-label: lst-run-all
#| lst-cap: "Master script for running the complete simulation workflow"
#| eval: false
#| echo: true
#| message: false

library(marginaleffects); library(simhelpers); library(rvinecopulib)
library(doParallel); library(doRNG); library(ggplot2); library(cowplot); 
library(dplyr)

source("03_run_simulation.R")
source("04_performance_metrics.R")
source("05_plots.R")
```

\normalsize

Thus, to reproduce the simulation from within the Nix shell, meaning after
running nix-build and nix-shell (@lst-run-simulation):

::: {#lst-run-simulation}
\small

``` bash
Rscript 06_run_all.R
```

Running the complete simulation workflow \normalsize
:::

Alternatively, individual scripts can be executed separately (i.e.,
`Rscript 03_run_simulation.R)`). Therefore, the key advantage of executing
within `nix-shell` is that all dependencies—R version, packages, and system
tools—match exactly those specified in `default.nix`.

## Additional Considerations for Advanced Workflows

### Workflow Orchestration: {targets} and {rixpress}

Complex simulation studies often benefit from workflow management systems that
track dependencies between computational steps, cache intermediate results, and
enable selective re-execution when inputs change. Two complementary approaches
exist within the Nix ecosystem: using {targets} inside a Nix environment, or
using {rixpress} to leverage Nix itself as the build automation tool.

#### Using {targets} Within Nix 

As mentioned, the {targets} package [@landau_2021] provides workflow
orchestration for R-based projects. This combination ensures both computational
reproducibility (via Nix controlling the environment) and computational
efficiency (via targets' intelligent caching). To integrate {targets} with Nix,
simply include "targets" in the `r_pkgs` parameter of `rix()`, **\[code listing?
(JG)\]**and execute the pipeline within `nix-shell` using
`Rscript -e 'targets::tar_make()'`. The {targets} metadata directory
(`_targets/`) should be excluded from version control while the `_targets.R`
configuration file should be committed alongside `default.nix`
[@rodrigues_baumann_2025]. This approach is ideal for projects that remain
within the R ecosystem and do not require different computational environments
for different pipeline steps (see {rix} documentation:
<https://docs.ropensci.org/rix/articles/z-advanced-topic-reproducible-analytical-pipelines-with-nix.html>).

#### Using {rixpress} for Polyglot Pipelines

The {rixpress} package [@rixpress], a sister package to {rix}, uses Nix itself
as the build automation tool rather than operating within a Nix environment.
Each pipeline step becomes a Nix derivation, providing hermetic builds with
sandboxed execution and content-addressable caching. The key advantage of
{rixpress} emerges in multi-language workflows: different steps can execute in
different Nix-defined environments (e.g., one step using R 4.2.0 with specific
packages, another using Python 3.12 with machine learning libraries, another
using Julia for numerical optimization). The package interface, inspired by
{targets}, uses functions like `rxp_r()`, `rxp_py()`, and `rxp_jl()` to define
pipeline steps, with automatic serialization handling data transfer between
languages. Objects are stored in the Nix store and can be inspected
interactively using helper functions like `rxp_read()` and `rxp_load()` (see
{rixpress} documentation:
<https://docs.ropensci.org/rixpress/articles/intro-concepts.html>).

## Converting Existing {renv} Projects

Many researchers have existing projects using {renv} for package management. The
`renv2nix()` function facilitates migration by reading an `renv.lock` file and
generating an equivalent Nix specification. This conversion is particularly
valuable for projects where {renv} encountered system dependency issues or where
stricter reproducibility guarantees are desired. However, researchers should
note that while {renv} snapshots R package versions, Nix additionally pins
system libraries and compilers, potentially exposing previously hidden
dependencies on system configuration [@rodrigues_baumann_2025] (see {rix}
documentation: <https://docs.ropensci.org/rix/articles/f-renv2nix.html>).

## Containerization with Docker

Institutions with existing Docker-based infrastructure may wish to combine Nix
with containers. While this might seem redundant—both technologies provide
isolation—the combination offers complementary benefits: Nix ensures
bit-reproducible builds across systems, while Docker provides a familiar
deployment mechanism for non-Nix-aware computing environments. The approach is
to use Nix as the base layer within a Docker container
[@rodrigues_baumann_2025]. This strategy is particularly relevant, for example,
for projects requiring deployment to cloud computing platforms or
high-performance computing clusters where Docker is the standard
containerization technology (see {rix} documentation:
<https://docs.ropensci.org/rix/articles/z-advanced-topic-using-nix-inside-docker.html>).

# Discussion

Reproducibility in computational research is often treated as a matter of
transparency—making data and code available. This tutorial has argued that
transparency alone is insufficient without the ability to reliably reconstruct
the computational environments in which analyses are executed. For simulation
studies in particular, where results depend critically on software versions,
system libraries, and random number generation, environment-level
reproducibility is not optional but essential.

By introducing Nix and the {rix} package, we demonstrated a practical and
accessible approach to fully specifying and rebuilding computational
environments for simulation-based research. This approach enables analyses and
manuscripts to be rerun identically across machines and over time, transforming
reproducibility from an aspirational goal into a verifiable property of the
research workflow.

Importantly, adopting environment reproducibility does not require abandoning
existing analytic practices. Nix is agnostic to programming language, editor,
workflow structure, and manuscript template, allowing researchers to retain
familiar tools while strengthening the reliability of their work. In this sense,
reproducible environments serve as enabling infrastructure—supporting, rather
than replacing, other best practices such as version control, workflow
orchestration, and transparent reporting.

If reproducibility is to function as a cornerstone of cumulative science, then
the ability to reconstruct computational environments must become a routine part
of methodological practice. Tools such as Nix and {rix} lower the barrier to
achieving this goal, making fully reproducible simulation research feasible
without requiring deep systems expertise. We hope this tutorial helps normalize
environment-level reproducibility as a standard component of rigorous
computational research in psychology and beyond.

{{< pagebreak >}}

# References

::: {#refs}
:::

{{< include appendix_a.qmd >}}

{{< include appendix_b.qmd >}}
