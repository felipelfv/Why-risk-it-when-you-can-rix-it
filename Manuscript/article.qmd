---
title: "Why Risk it, When You Can {rix} it: A Tutorial for Computational Reproducibility Focused on Simulation Studies"
shorttitle: "Reproducibility with {rix}"
author:
  - name: Felipe Fontana Vieira
    corresponding: true
    orcid: 0009-0006-0949-6569
    email: felipe.fontanavieira@ugent.be
    affiliations:
      - name: Ghent University 
        department: Department of Data Analysis 
    roles:
      - Conceptualization
      - Methodology
      - Software
      - Data curation
      - Formal analysis
      - Visualization
      - Writing - original draft
      - Writing - review & editing
  - name: Jason Geller
    orcid: 0000-0002-7459-4505
    affiliations:
      - name: Boston College
        department: Department of Psychology and Neuroscienc
    roles: 
      - Conceptualization
      - Supervision
      - Validation 
      - Writing - review & editing
  - name: Bruno Rodrigues
    orcid: 0000-0002-3211-3689
    affiliations:
      - name: Ministry of Research and Higher Education, Luxembourg
        department: Statistics and Data Strategy Departments
    roles: 
      - Conceptualization
      - Software
      - Writing - review & editing
      
abstract: |
  Computational reproducibility remains limited in psychological research,
  despite widespread norms for sharing data and analysis code. One reason is
  that reproducibility exists on a continuum, ranging from partial
  transparency—such as providing scripts or software version numbers—to fully
  executable research compendia that regenerate all results from raw code. In
  this article, we introduce Nix and the {rix} R package as a practical
  framework for achieving full computational reproducibility in simulation-based
  research. We provide a step-by-step tutorial demonstrating how {rix} can be
  used to define, build, and share isolated, project-specific software
  environments that precisely capture R versions, package dependencies, system
  libraries, and integrated development environments. We further illustrate
  this workflow by reproducing a complete manuscript using Quarto and the
  {apaquarto} extension, showing how analyses, figures, and text can be
  regenerated in a single, executable pipeline. Together, these tools lower the
  technical barrier to robust, end-to-end reproducibility and offer a scalable
  solution for simulation studies and methodological research in psychology and
  related fields.

keywords: [reproducibility, Nix, simulation studies, R, computational methods]
word-count: true
author-note:
  disclosures:
    gratitude: "We would like to thank Julia Rohrer and Ole Schacht for their helpful feedback 
    on earlier versions of this manuscript."
    conflict-of-interest: "The authors have no conflicts of interest to declare."
    data-sharing: "Data and materials from this study can be accessed at https://github.com/felipelfv/Why-risk-it-when-you-can-rix-it."
format:
  apaquarto-pdf:
    document-mode: man
    keep-tex: true
    include-in-header:
      text: |
        \usepackage{setspace}
        \usepackage{etoolbox}
        \raggedbottom
        % Single-space code blocks
        \AtBeginEnvironment{Shaded}{\singlespacing}
        \AtBeginEnvironment{verbatim}{\singlespacing}
  apaquarto-html:
    embed-resources: true
    toc: true
bibliography: references.bib
editor: 
  markdown: 
    wrap: 80
---

```{r}
#| label: load-packages
#| echo: false
#| message: false

# required packages
library(marginaleffects)
library(simhelpers)
library(rvinecopulib)
library(doParallel)
library(doRNG)
library(ggplot2)
library(cowplot)
library(knitr)
library(dplyr)

```

Psychological science is in the midst of a credibility revolution, which has
prompted substantial progress in how research is conducted and evaluated
[@vazire2018]. Yet, despite notable progress, a key cornerstone of science,
reproducibility (i.e., the ability to precisely reproduce the results of a study
or studies based on provided data, code, materials, and software/hardware)
remains limited [@hardwicke2020]. Hence, ensuring reproducibility remains an
open and pressing challenge for psychological science.

Addressing this gap is complicated by the fact that reproducibility is not a
binary feature but instead exists along a continuum [@peng_2011]. At the lower
end, reproducibility may be interpreted as sharing only a manuscript. Further
along the spectrum, it may involve providing partial code, complete analysis
scripts, or publicly accessible datasets. At the highest level, reproducibility
entails documenting a fully specified computational environment that allows
others to recreate identical results—from raw data to final manuscript
output—with minimal friction. As a result, researchers may implicitly target
different points on this continuum, and efforts to improve reproducibility can
diverge substantially in both goals and implementation.

Open science initiatives have made considerable progress in encouraging movement
along this spectrum. For example, journals now offer open-science badges
[@kidwell_etall_2016], best practices have been developed to make data sharing
routine [@levenstein_lyle_2018], and platforms such as the Open Science
Framework (OSF) provide infrastructure for storing and sharing research
materials [@nosek_etall_2015]. However, these efforts primarily target the
lower to middle portions of the continuum, emphasizing *what* is shared rather
than *how* shared materials can be executed in practice.

Data and code are never fully self-sufficient to reproduce a set of findings
[@peikert_brandmaier_2021; @epskamp_2019; @wiebels_moreau_2021; 
@ziemann_etall_2023]. Assuming the data and code are error-free, 
reproducibility depends on a hierarchy of software
components—collectively referred to as *dependencies*—including the programming
language version, the packages used in the analysis, and the system libraries on
which those packages rely. When these dependencies differ from those used in the
original analysis, code may fail, behave inconsistently across machines, or
yield conflicting numerical results [@baker_etall_2024; @hodges_etall_2023;
@glatard_etall_2015; @nosek_etall_2022]. These issues are particularly acute for
simulation studies, which rely on complex codebases, versioned dependencies, and
intricate software configurations [@luijken_etall_2024; @siepe_etall_2024].

To make this concrete, we use *computational environment* to refer to the
complete software context required for an analysis to run successfully—the
programming language version, package versions, system libraries, and operating
system [@rodrigues_2023; @rodrigues_baumann_2026_polyglot]. We define
*computational environment reproducibility* as the ability to reconstruct this
entire set of software dependencies on any machine and at any future time, such
that executing the same code yields the same numerical results. Empirical
assessments show that current practice falls short of this ideal.
@siepe_etall_2024 report that nearly two-thirds of simulation studies in
psychology provide no accompanying code, and among those that do, documentation
of the computational environment is rarely included. This gap is consequential:
simulation studies inform methodological recommendations, meaning that
insufficient reproducibility undermines confidence in those recommendations
[@luijken_etall_2024; @white_etall_2024].

Arguably, these challenges persist because researchers must navigate a
fragmented landscape of solutions, each addressing only part of the problem.
Package-level managers such as {renv} [@ushey_2024] and {groundhog}
[@simonsohn_2020] stabilize R package versions but do not manage the R
interpreter itself or the system-level libraries those packages depend on.
Workflow orchestration tools such as {targets} [@landau_2021] and Make
[@feldman_1979] support reproducibility in a different sense: they specify the
structure of an analysis by formalizing the order in which steps should run and
by tracking dependencies among intermediate results. These tools clarify *how*
an analysis proceeds, but they assume that the software stack required to run
each step is already stable. Containerization tools such as Docker, including
R-focused implementations like Rocker [@boettiger_2015;
@boettiger_eddelbuettel_2017] offer a more comprehensive approach by bundling
the full environment (i.e., operating system, system libraries, interpreter 
versions, and packages) into a single executable image. Yet their use requires 
familiarity with Linux system administration, and even containerization may 
suffer from temporal drift when Dockerfiles rely on mutable upstream 
repositories [@malka2024]. For a detailed comparison of these tools and 
their limitations, see @rodrigues_baumann_2026_polyglot. Researchers thus 
face a difficult choice between solutions that are accessible but incomplete 
or approaches that are powerful but demand technical expertise.

In this article, therefore, we focus specifically on computational environment
reproducibility as the foundation upon which other reproducibility practices
depend. For that, we introduce Nix [@dolstra_etall_2004], a functional 
software^[*Functional* is used in the sense of functional programming (pure 
functions, immutability, explicit inputs), not in the colloquial sense of *working*.]
ecosystem designed to make software installation deterministic, and {rix}
[@rodrigues_baumann_2025], an R interface that allows researchers to use Nix
without needing deep knowledge of its underlying language or infrastructure. Our
main objective with the tutorial is not to introduce a specific workflow
orchestration system or to prescribe a particular analytic structure. Instead,
we aim to show what Nix and {rix} are and how they can establish a stable,
cross-platform environment within which any simulation study—whether organized
in simple, documented script sequences (e.g., .R files that `source()` others),
through more formal orchestration tools (e.g., {targets}) or embedded as code
chunks in `.Rmd` or `.qmd`—can be executed reliably.

We illustrate these ideas through a reproducible simulation study conducted in
R, culminating in this automated APA-formatted manuscript generated with
`apaquarto` [@schneider_2024]. Although the example centers on R because of its
prominence in psychological methodology, the principles underlying environment
reproducibility with {rix} apply equally to other languages, including Python 
and Julia, and to different development environments such as RStudio, VS Code, 
Emacs, or Positron. At the end of the article, we also briefly introduce 
{rixpress} [@rixpress], a workflow orchestration tool that builds on 
Nix to coordinate pipelines across R, Python, and Julia.

# A Practical Example: Setting up a Reproducible Simulation Study with {rix}

Imagine you have just been awarded a grant to conduct a large-scale simulation
study. The study is designed to evaluate the performance of a statistical
estimator under varying data-generating conditions (see Appendix A for full
technical details). This tutorial is organized around this scenario. We use this
example to ground our discussion into a typical methods section, but readers can
follow the tutorial without engaging deeply with the simulation itself.

In practice, simulation studies are typically organized into multiple component
files, each corresponding to a distinct analytical stage, a modular structure
that facilitates development and debugging. In our case, the simulation is
organized into five sequential scripts: data generation
(`01_data_generation.R`), model specification (`02_models.R`), simulation
execution (`03_run_simulation.R`), performance metric calculation
(`04_performance_metrics.R`), and results visualization (`05_plots.R`). However,
because our focus is on the reproducibility of the *entire manuscript*, we embed
all code directly within this document as executable chunks in a single `.qmd`
file. When rendered, the simulation runs from start to finish, producing results
and figures automatically. This approach would be impractical for many
real-world simulation studies, which are often too computationally intensive. We
return to this trade-off later in the tutorial.

Now suppose a researcher attempts to reproduce the simulation results reported
in the article. What might prevent them from obtaining identical outcomes? The
natural first concern is the version of packages. Installing R packages at a
later time may lead to errors if functions have been renamed or deprecated
(e.g., `lavaan:::lav_utils_get_ancestors` renamed to
`lavaan:::lav_graph_get_ancestors`), or to subtly different results due to
changes in default settings (e.g., `stringsAsFactors` defaulting to FALSE as of
R 4.0) or numerical implementations. Beyond package versioning, many packages
rely on system-level libraries that must be installed separately from R. Our
simulation illustrates this dependency structure directly: the {rvinecopulib}
package interfaces with a C++ backend and links against external libraries such
as Boost, Eigen, and RcppThread [@rvinecopulib].

The R language version introduces another layer of dependency. Code written for
R 4.0 may rely on syntax or functionality that is unavailable in earlier
versions (e.g., the native pipe `|>` introduced in R 4.1). More subtly, changes
to R’s random number generation across major versions mean that identical code
executed with the same seed can nevertheless produce different random sequences
[@ottoboni_stark_2018]. For simulation studies, where specific random draws
often underpin reported results, this version sensitivity is consequential.

Finally, when analyses are embedded in a literate programming workflow (i.e.,
documents that combine narrative text and executable code; dynamic document
generation) additional layers of software dependencies arise. For example,
rendering R Markdown (`.rmd`) or Quarto documents (`.qmd`) requires both a
document conversion tool (e.g., Pandoc, which converts `.rmd` or `.qmd` files
into formats such as PDF or HTML) and a typesetting system such as a LaTeX
distribution or Typst. Each of these components introduces its own versioning
constraints and platform-specific installation requirements. Taken together,
these layers highlight that reproducibility depends not only on code and data,
but also on the broader computational environment in which analyses are
executed.

# Nix and {rix}: A Comprehensive Solution

A potential solution to this problem is Nix [@dolstra_etall_2004], a software 
ecosystem whose primary concern is reproducible, declarative builds. To achieve this
Nix is a programming language, a build tool and a package manager. This article
focuses mostly on Nix the package manager. Most package managers 
(think of Apple’s or Android’s app stores) are imperative: they modify a system’s
state as they install or update software. Nix, in contrast, treats build 
instructions and dependencies as immutable, enforcing reproducible, declarative, 
and isolated environments across platforms. This allows researchers to specify 
exactly which versions of programming languages, packages, and system libraries 
an analysis requires, and to recreate that environment reliably on any machine.

Unlike familiar tools such as `install.packages()` in R, `apt-get` on Linux, or
`uv` in Python—which typically manage only a single layer of the software
stack—Nix handles language versions, package versions, and system-level
dependencies within a single framework [@rodrigues_baumann_2025]. Rather than
installing software into shared system directories, Nix builds each environment
as an explicit, self-contained specification. As a result, multiple environments
can coexist without conflict, and analyses can be rerun months or years later
under identical computational conditions.

This unified approach directly addresses the fragmented landscape described
above. Where researchers would otherwise need to coordinate separate tools for
package management, interpreter versions, and system dependencies, Nix brings
all three together within a single declarative model, lowering the barrier to
fully reproducible computational workflows.

## Core Principles

Rather than installing software into global directories (e.g., `/usr/lib`), Nix
places every package in its own directory under `/nix/store`. Each package path
contains a cryptographic hash representing its precise inputs—source code,
dependencies, and build instructions. Because these paths are content-addressed,
multiple versions of the same software can coexist without conflict. A
researcher can, for example, maintain projects requiring R 4.1.0 and R 4.3.3
side by side, or use different package versions across analyses, switching
between them seamlessly [@rodrigues_baumann_2025].

The Nix ecosystem is built around nixpkgs, a version-controlled repository
comprising more than 120,000 packages, including nearly all of CRAN and
Bioconductor. By pinning a specific commit or date, researchers freeze the
entire software stack (i.e., R itself, R packages, and all system libraries) 
at the time of this writing. This eliminates the system-dependency problems 
that tools like {renv} cannot address [@rodrigues_baumann_2025]. This 
architecture also ensures stability over time. Empirical work has shown strong 
rebuildability and reproducibility rates for historical nixpkgs snapshots
[@rodrigues_baumann_2026_polyglot]. Combined with binary caches, which often
allow environments to materialize in seconds, Nix becomes practical for
interactive research workflows [@rodrigues_baumann_2025].

## The {rix} Package: R Interface to Nix

As previously stated, Nix is also a functional programming language. 
Because the Nix package manager is declarative, it requires expressions 
written in this language to install software. However, since Nix is a functional 
programming language unfamiliar to most researchers, we recommend using {rix} to lower this barrier. 
The {rix} package provides an R-native interface: a single call to `rix()` generates 
complete Nix configurations from standard R syntax, specifying R versions, 
CRAN packages, system libraries, and even Python or Julia components when required. 
Users never need to read or write Nix code directly, as {rix} translates automatically [@rodrigues_baumann_2025].

A key feature of {rix} is its integration with rstats-on-nix, a
community-maintained fork offering daily CRAN snapshots and weekly tested
environments on Linux and macOS. Researchers can request, for example,
`rix(date = "2024-12-14")` to obtain a validated and reproducible environment
without manually assessing compatibility. After the configuration is generated,
`nix_build()` instantiates the environment, and binary caches typically allow
this to complete within seconds [@rodrigues_baumann_2025].

Although Nix is capable of replacing tools like Docker for isolation or {renv}
for package management, it does not require an all-or-nothing transition.
Researchers can adopt it gradually and use it alongside familiar tooling. For
instance, by building Docker images with Nix, converting existing {renv}
lockfiles, or running {targets} pipelines within a Nix-defined environment
[@rodrigues_baumann_2025]. This allows Nix to strengthen reproducibility while
preserving established workflows. We will come back to this after the tutorial.

## Step 1: Installing Nix and {rix}

Before proceeding, both Nix and the {rix} R package need to be installed.
Installation procedures differ across operating systems (Windows via WSL2,
Linux, and macOS), and detailed, up-to-date instructions are maintained in the
official {rix} documentation:

-   **Linux and Windows (WSL2)**:
    <https://docs.ropensci.org/rix/articles/b1-setting-up-and-using-rix-on-linux-and-windows.html>
-   **macOS**:
    <https://docs.ropensci.org/rix/articles/b2-setting-up-and-using-rix-on-macos.html>

Once Nix is installed[^1], there are two ways to access {rix}, depending on
whether R is already installed on your system. In this tutorial, we proceed as
if R was already installed[^2] (@lst-install-rix-cran):

[^1]: It is worth noting that {rix} can generate Nix expressions even without
    Nix installed on your system—you can write a `default.nix` file without Nix,
    but you cannot build or enter the resulting environment unless Nix is
    installed [@rodrigues_baumann_2025].

[^2]: We, however, recommend uninstalling your local R and letting Nix manage
    R, R packages, and other tools entirely. This approach avoids potential
    conflicts between system-installed and Nix-managed software, an issue we
    will illustrate later in this tutorial. See
    <https://docs.ropensci.org/rix/articles/setting-up-linux-windows.html#case-1-you-dont-have-r-installed-and-wish-to-install-it-using-nix-as-well>
    for more details.

```{r}
#| lst-label: lst-install-rix-cran
#| lst-cap: "Installing {rix} from CRAN or developmental version"
#| eval: false
#| echo: true
# CRAN version
install.packages("rix") 
# Development version
install.packages(
  "rix",
  repos = c(
    "https://ropensci.r-universe.dev"
  )
)
```

## Step 2: Specifying the Computational Environment {#sec-specifying-environment}

After that, we need to establish a reproducible environment by creating a script
that will generate the environment specification. We recommend creating a file
named `generate-env.R` (or similar) in the project directory. This script will
use the `rix()` function from the {rix} package to produce a `default.nix`
file—a declarative specification that precisely defines all software
dependencies required for the project.

In our case, where we use literate programming for generating the manuscript, we
implement the following environment specification, which can be found on the
GitHub repository as a file named `gen-env.R` (@lst-rix-env-manuscript):

\small

```{r}
#| lst-label: lst-rix-env-manuscript
#| lst-cap: "Environment specification for the manuscript using rix()"
#| eval: false
#| echo: true

library(rix)

rix(
  date = "2026-01-14",
  r_pkgs = c(
    "rix", "quarto", "knitr", "marginaleffects",
    "simhelpers", "ggplot2", "doParallel", "doRNG", "cowplot",
    "dplyr", "svglite", "rvinecopulib"
  ),
  system_pkgs = c("quarto"),
  tex_pkgs = c("amsmath", "ninecolors", "apa7", "scalerel",
    "threeparttable", "threeparttablex", "endfloat", "environ",
    "multirow", "tcolorbox", "pdfcol", "tikzfill", "fontawesome5",
    "framed", "newtx", "fontaxes", "xstring", "wrapfig", "tabularray",
    "siunitx", "fvextra", "geometry", "setspace", "fancyvrb", 
    "anyfontsize"
  ),
  ide = "rstudio",
  project_path = ".",
  overwrite = TRUE
)
```

\normalsize

Thus, note that we have more than just the R packages specified for the
simulation scripts. This happens because we also included what is needed for the
manuscript generation, not solely for the simulation code. In Appendix B, we
mention more specifically the reasons for adding each package in `r_pkgs()` and
`tex_pkgs()`. For now, we focus more on clarifying the different arguments for
the `rix()` function.

### The Environment Generation Script

The `rix()` function[^3] constructs this specification through a series of
parameters that collectively describe the computational environment. Each
parameter serves a distinct purpose in defining the environment's
characteristics.

[^3]: For an overarching information on the function `rix()`, we suggest the
    following {rix} documentation:
    <https://docs.ropensci.org/rix/articles/project-environments.html>

#### Specifying the R version

Researchers must first determine which version of R to use. This can be
accomplished in two ways: The `r_ver` argument accepts an exact version string
(e.g., "4.3.3") or special designations such as "latest-upstream" for the most
recent stable release. Alternatively, the `date` argument specifies a particular
date (e.g., "2024-11-15"), which ensures that R and all packages correspond to
the versions available on that date. The date-based approach is generally
preferable for reproducibility, as it captures a complete snapshot of the R
ecosystem at a single point in time. For this tutorial, as shown on top, we use
the `date` parameter to ensure temporal consistency across all software
components [@rodrigues_baumann_2025] (see {rix} documentation for more:
<https://docs.ropensci.org/rix/articles/project-environments.html>).

#### Declaring R package dependencies

The `r_pkgs` argument accepts a character vector listing all required R packages
by their CRAN names. These packages will be installed from the version
repository corresponding to the specified date or R version. It is important to
list all packages that the analysis will load directly; dependencies of these
packages are automatically resolved by Nix. For packages requiring specific
versions not corresponding to the chosen date, researchers can specify exact
versions using the syntax `"packagename@version"` (e.g., `"ggplot2@2.2.1"`). For
packages available only on GitHub or other Git repositories, the `git_pkgs`
argument accepts a list structure containing repository URLs and specific commit
hashes. For example:

\small

```{r}
#| lst-label: lst-git-pkgs-example
#| lst-cap: "Example for the git_pkgs argument"
#| eval: false
#| echo: true

git_pkgs = list(
  package_name = "marginaleffects",
  repo_url = "https://github.com/vincentarelbundock/marginaleffects",
  commit = "304bff91dc31ae28b227a8485bfa4f7bdc86d625"
)
```

\normalsize

This ensures that exact development versions are obtained
[@rodrigues_baumann_2025]. For our simulation study, all packages were used with
their CRAN versions (see {rix} documentation for more details:
<https://docs.ropensci.org/rix/articles/installing-r-packages.html>).

#### Including system-level dependencies

Many R-based workflows require tools beyond R packages. The `system_pkgs`
parameter specifies system-level software such as Quarto for document
generation, Git for version control, or Pandoc for document conversion.
Critically, we include Quarto as a system package because this tutorial
demonstrates full computational reproducibility—not merely of the simulation
code, but of the complete manuscript itself. Our manuscript uses the `apaquarto`
extension for APA formatting, stored in the project's `_extensions/` directory
[@rodrigues_baumann_2025] (see {rix} documentation for more:
<https://docs.ropensci.org/rix/articles/installing-system-tools.html>).

#### Specifying LaTeX packages

The `tex_pkgs` parameter specifies LaTeX packages needed for PDF compilation.
When any packages are listed, Nix automatically includes a minimal TexLive
distribution (`scheme-small`) as a base, to which the specified packages are
added. Determining the required LaTeX packages may involve some trial and
error—Quarto's error messages during, for example, the PDF rendering indicate
which packages are missing, and these can then be added to `tex_pkgs` (see {rix}
documentation for more:
<https://docs.ropensci.org/rix/articles/installing-system-tools.html>). The tex
packages included in the above code offer a nice starting point for researchers
wanting to create manuscripts using LaTeX.

#### Configuring the development environment

The `ide` parameter controls whether an integrated development environment (IDE)
is included in the Nix environment, allowing users to interactively develop and
run code within their editor of choice. When `ide` is specified, the project can
be opened directly in the corresponding IDE, with all dependencies provided by
the Nix environment. For example, setting `ide = "rstudio"` installs a
project-specific version of RStudio inside the Nix environment. This is required
for RStudio because, unlike most other editors, it cannot attach to an external
Nix shell unless it is itself installed via Nix. On macOS, RStudio is only
available through Nix for R versions 4.4.3 or later (or environments dated
2025-02-28 or later); for earlier R versions, alternative editors must be used.
Other supported IDEs include Positron (`ide = "positron"`), Visual Studio Code
(`ide = "code"`), and command-line interfaces such as Radian (`ide = "radian"`).
These tools may either be installed directly within the Nix environment using
the `ide` parameter, or users may rely on an existing system installation by
setting `ide = "none"` (or `ide = "other"`) and configuring `direnv` to
automatically load the Nix environment when the project directory is opened[^4].
All IDEs installed via Nix are project-specific and do not interfere with
system-wide installations. Detailed configuration instructions are provided in
the {rix} documentation:
<https://docs.ropensci.org/rix/articles/configuring-ide.html>

[^4]: **`direnv`** is a lightweight utility that integrates with the user's
    shell and automatically loads project-specific environment settings when
    navigating into a directory (via a `.envrc` file), and unloads them when
    leaving. This makes environment activation implicit and reduces the risk of
    running analyses in the wrong software context.

#### Setting file output parameters

The `project_path` parameter indicates where the `default.nix` file should be
written ("." denotes the current directory), while `overwrite` controls whether
an existing file should be replaced. Adding to this, setting `print = TRUE`,
which is another argument, displays the generated specification in the console
for immediate verification [@rodrigues_baumann_2025].

#### Multi-language environment support

While this tutorial focuses on R, researchers working across multiple
programming languages can include Python or Julia in their environments. The
`py_conf` parameter accepts a list specifying a Python version and required
packages (@lst-python-pkgs-example). Similarly, `jl_conf` enables Julia package
installation. This capability is particularly useful, for example, for projects
requiring statistical computing in R alongside machine learning pipelines in
Python or numerical optimization in Julia [@rodrigues_baumann_2025] (see {rix}
documentation for more:
<https://docs.ropensci.org/rix/articles/installing-r-packages.html>).

```{r}
#| lst-label: lst-python-pkgs-example
#| lst-cap: "Including Python packages"
#| eval: false
#| echo: true

py_conf = list(py_version = "3.12", py_pkgs = c("polars", "pandas"))

```

## Generating the Environment Specification

After defining the computational environment, the `rix()` function must be
executed to generate the `default.nix` file. This can be done interactively by
running `rix()` in an R console (@lst-rix-env-manuscript). The resulting 
`default.nix` file serves as the complete environment
specification and contains all information required to recreate the project in a
fully reproducible manner.

## Step 3: Building and Using the Reproducible Environment

Once Step 2 is complete, build the reproducible environment by navigating to the
study directory in a terminal. You may use either the integrated Terminal in
RStudio (Tools → Terminal → New Terminal) or an external system terminal from
which you are running the Nix project. From the study directory, run the
following command (@lst-nix-build):

::: {#lst-nix-build}
\small

``` bash
user@computer Why-risk-it-when-you-can-rix-it % nix-build
```

Building the Nix environment \normalsize
:::

The expected output should look similar to (@lst-nix-build-output):

::: {#lst-nix-build-output}
\small

``` bash
unpacking 'https://github.com/rstats-on-nix/nixpkgs/archive/2025-08-25.tar.gz'
  into the Git cache...
warning: ignoring untrusted substituter...
warning: ignoring the client-specified setting...
/nix/store/qa7fq20m2f94szsnqzciwv8h4n81w43v-nix-shell
```

Expected output from `nix-build` \normalsize
:::

This command builds the environment according to the specification. The first
execution will download and install all required packages, which may take a few
minutes depending on network speed and system resources. Subsequent builds use
cached packages and complete in seconds. Upon successful completion, a path to
the constructed environment in the Nix store is printed (here,
`/nix/store/qa7fq20m2f94szsnqzciwv8h4n81w43v-nix-shell`), and a symbolic link
named `result` appears in the project directory pointing to this location.

Note that the warnings indicate that you are not configured as a trusted user,
so Nix cannot use the rstats-on-nix binary cache and will instead compile
packages from source, which is slower. To enable binary caching, install the
cachix client and configure the rstats-on-nix cache. See
<https://docs.ropensci.org/rix/articles/binary-cache.html> for instructions.

To activate the environment, run (@lst-nix-shell):

::: {#lst-nix-shell}
\small

``` bash
user@computer Why-risk-it-when-you-can-rix-it % nix-shell
```

Activating the Nix environment \normalsize
:::

The expected output (if you have configured yourself as a trusted user,
otherwise the same warnings will appear) should look similar to
(@lst-nix-shell-output):

::: {#lst-nix-shell-output}
``` bash
unpacking 'https://flakehub.com/f/DeterminateSystems/nixpkgs-weekly/...' 
  into the Git cache...
[nix-shell:~/AMPPS/Why-risk-it-when-you-can-rix-it]$
```

Expected output from `nix-shell`
:::

This command drops the user into a shell where all specified packages and tools
are available. The shell prompt changes to indicate that a Nix environment is
active (here, `[nix-shell:~/Desktop/AMPPS/Why-risk-it-when-you-can-rix-it]$`).
To verify that R is being provided by Nix rather than a system installation, run
`which R`. This should return a path within `/nix/store/`. Moreover, from within
the Nix shell, users can launch their IDE by typing its name (e.g., `rstudio` or
`positron`), which opens the IDE with the Nix environment active
(@lst-shell-rs)[^5]

[^5]: Please note that activating an RStudio instance via Nix does **not**
    automatically open the specific project directory you are working in. We
    therefore recommend creating an **RStudio project file (`.Rproj`)** and
    opening that file when using Nix to ensure that RStudio is correctly
    associated with the intended project and environment.

::: {#lst-shell-rs}
\small

``` bash
[nix-shell:~/AMPPS/Why-risk-it-when-you-can-rix-it]$ rstudio
```

Activating RStudio \normalsize
:::

## Reproducing the Complete Manuscript[^6]

[^6]: See the {rix} documentation for more:
    <https://docs.ropensci.org/rix/articles/literate-programming.html>

Within the nix shell, one is able to render the manuscript as follows in the
terminal (@lst-quarto-render):

::: {#lst-quarto-render}
\small

``` bash
[nix-shell:~/AMPPS/Why-risk-it-when-you-can-rix-it]$ 
  quarto render Manuscript/article.qmd
```

Rendering the manuscript with Quarto \normalsize
:::

This command executes all code chunks in the manuscript, incorporates results 
(e.g., Table 1) and figures (e.g., Figure 1), and generates a formatted PDF 
following APA style guidelines via the `apaquarto` extension [@schneider_2024]. 
This extension is saved in the project repo already. 
To download this extension for your own work you can install the extension 
by using the terminal (@lst-apaquarto-install):

::: {#lst-apaquarto-install}
\small

``` bash
user@computer Why-risk-it-when-you-can-rix-it % quarto use template 
  wjschne/apaquarto
```

Installing the apaquarto extension \normalsize
:::

or in the console (@lst-apaquarto-r):

\small

```{r}
#| lst-label: lst-apaquarto-r
#| lst-cap: "Installing the apaquarto extension from R"
#| eval: false
#| echo: true

quarto::quarto_use_template("wjschne/apaquarto")
```

\normalsize

The final document (.docx, .pdf, or .html) is saved directly in the project
folder[^7]. Because Quarto is installed as a system-level package in our Nix
specification, the rendering occurs entirely within a fully reproducible
environment, ensuring consistent output across machines regardless of local
software configurations. If desired, the manuscript can also be reproduced
interactively by opening the project folder in the user’s preferred IDE.

[^7]: Although we use `apaquarto` in this example, many alternative manuscript
    templates are available, and Nix is agnostic to the specific template
    employed, provided the necessary extensions are installed.

At this point, it is also worth noting that Nix shells do not fully isolate
you from your existing system by default (as mentioned in footnote 2). For R
users, this has a practical implication: packages installed in your regular R
library (outside of Nix) could potentially be loaded when running R from within
the Nix environment. 
The {rix} package addresses this automatically—when you call `rix()`, it also 
executes `rix_init()`, which creates a project-specific `.Rprofile`. 
This file configures R to ignore external package libraries and also disables 
`install.packages()` within the environment. The rationale is straightforward: 
any new packages should be added to `default.nix` and the environment rebuilt, 
preserving full reproducibility [@rodrigues_baumann_2025]. However, for 
stricter isolation[^8] that also prevents access to other system programs not 
specified in `default.nix`, use the `--pure` flag (@lst-nix-shell-pure):

[^8]: For example, when preparing this manuscript without the `--pure` flag,
    `quarto render` worked successfully. However, when using the `--pure` flag,
    the build failed. Running `quarto check` from within the Nix shell (i.e.,
    `nix-shell --run "quarto check"`) revealed that Quarto was still accessing
    the system's LaTeX installation (`/Library/TeX/texbin`) rather than being
    restricted to only what was specified in `default.nix`.

::: {#lst-nix-shell-pure}
\small

``` bash
nix-shell --pure
```

Activating the Nix environment with strict isolation \normalsize
:::

```{r}
#| lst-label: lst-data-generation
#| lst-cap: "Data Generation Function"
#| echo: false
#| message: false

# fixed data-generating process (DGP) parameters
# parameters for treatment assignment model: X1 = alpha0 + alpha1*X2 + alpha2*X2^2 + error
alpha0 <- 0      # intercept for treatment equation
alpha1 <- 0.5    # linear effect of confounder X2 on treatment X1
alpha2 <- 0.2    # quadratic effect of confounder X2 on treatment X1

# parameters for outcome model: logit(P(Y=1)) = beta0 + beta1*X1 + gamma1*X2 + gamma2*X2^2
beta0 <- -0.5    # intercept for outcome equation
beta1 <- 0.7     # causal effect of treatment X1 on outcome Y (parameter of interest)
gamma1 <- -0.4   # linear effect of confounder X2 on outcome Y

# function to generate one dataset
# arguments:
#   n: sample size
#   gamma2: quadratic effect of confounder on outcome (varies by condition)
gen_data <- function(n, gamma2) {
  # generate independent uniform pairs using {rvinecopulib} (C++ backend)
  # this is equivalent to rnorm() but demonstrates system-level dependencies
  u <- rbicop(n, "indep", 0, numeric(0))

  # transform uniform marginals to standard normal via inverse CDF
  X2 <- qnorm(u[, 1])        # confounder
  error_X1 <- qnorm(u[, 2])  # error term for treatment equation

  # generate treatment variable (confounded by X2)
  X1 <- alpha0 + alpha1 * X2 + alpha2 * X2^2 + error_X1

  # compute true linear predictor for outcome model
  eta_true <- beta0 + beta1 * X1 + gamma1 * X2 + gamma2 * X2^2

  # convert to probability via logistic function
  p <- plogis(eta_true)

  # generate binary outcome
  y <- rbinom(n, size = 1, prob = p)

  # dataset with outcome and predictors
  data.frame(y = y, x1 = X1, x2 = X2)
}
```

```{r}
#| lst-label: lst-model-estimation
#| lst-cap: "Model Estimation Functions"
#| echo: false
#| message: false

# function to run one simulation replicate using the MISSPECIFIED model
# arguments:
#   n: sample size
#   gamma2: quadratic effect of confounder on outcome
run_one_rep <- function(n, gamma2) {
  # generate dataset for this replicate
  dat <- gen_data(n, gamma2)

  # fit misspecified model (omits X2^2 term, causing residual confounding)
  fit_misspec <- glm(y ~ x1 + x2, data = dat, family = binomial())

  # compute average causal effect (ACE) using {marginaleffects}
  ace_est <- avg_slopes(fit_misspec, variables = "x1")

  # point estimate, standard error, and 95% CI bounds
  data.frame(
    est = ace_est$estimate[1],
    se = ace_est$std.error[1],
    lo = ace_est$conf.low[1],
    hi = ace_est$conf.high[1]
  )
}

# function to compute the "true" ACE using the correctly specified model
# uses a large sample to approximate the population quantity
# arguments:
#   gamma2_val: quadratic effect of confounder on outcome
#   seed: random seed for reproducibility
compute_true_ace <- function(gamma2_val, seed = 12345) {
  set.seed(seed)

  # generate large dataset (n = 200,000) to approximate population
  bigdat <- gen_data(2e5, gamma2_val)

  # fit correctly specified model (includes X2^2 term)
  fit_correct <- glm(y ~ x1 + x2 + I(x2^2), data = bigdat, family = binomial())

  # true ACE estimate
  avg_slopes(fit_correct, variables = "x1")$estimate[1]
}
```

```{r}
#| lst-label: lst-main-simulation-loop
#| lst-cap: "Main Simulation"
#| echo: false
#| message: false
#| output: false

# simulation parameters
nsim <- 100          # replications per condition
ncore <- 1           # CPU cores for parallel processing
master_seed <- 12345 # seed 

# simulation factors
n_values <- c(50, 100, 2000)              # sample sizes to evaluate
gamma2_values <- c(0, 0.3, 0.8)           # confounding severity levels
gamma2_labels <- c("none", "mild", "severe")  # labels for gamma2 levels

# full factorial design (3 x 3 = 9 conditions)
designs <- expand.grid(
  n = n_values,
  gamma2 = gamma2_values,
  stringsAsFactors = FALSE
)

# descriptive labels for confounding severity
designs$confound_label <- factor(
  designs$gamma2,
  levels = gamma2_values,
  labels = gamma2_labels
)

# compute true ACE values for each gamma2 condition
# these serve as the reference for evaluating estimator performance
cat("Computing true ACE values\n")
true_ACE_values <- sapply(gamma2_values, compute_true_ace)
names(true_ACE_values) <- as.character(gamma2_values)
print(true_ACE_values)

# parallel processing backend
cl <- makeCluster(ncore)
registerDoParallel(cl)

# run simulation across all conditions
all_results <- vector("list", nrow(designs))

for (k in seq_len(nrow(designs))) {
  # extract current condition parameters
  n_k <- designs$n[k]
  gamma2_k <- designs$gamma2[k]
  label_k <- as.character(designs$confound_label[k])

  cat("\nCondition:", label_k, "(gamma2 =", gamma2_k, "), n =", n_k, "\n")

  # true ACE for this condition
  true_ACE_k <- true_ACE_values[as.character(gamma2_k)]

  # run nsim replications in parallel with reproducible RNG via {doRNG}
  res_list <- foreach(
    i = seq_len(nsim),
    .packages = c("marginaleffects", "rvinecopulib")
  ) %dorng%
    {
      run_one_rep(n_k, gamma2_k)
    }

  # combine replications into data frame
  res <- do.call(rbind, res_list)

  # add condition metadata to results
  res$true_param <- true_ACE_k
  res$n <- n_k
  res$gamma2 <- gamma2_k
  res$confound_label <- label_k

  all_results[[k]] <- res
}

# clean up parallel backend
stopCluster(cl)

# all conditions into single data frame
sim_results <- do.call(rbind, all_results)
```

```{r}
#| lst-label: lst-performance-metrics
#| lst-cap: "Performance Metrics"
#| echo: false
#| message: false

# unique simulation conditions
conditions <- unique(sim_results[, c("n", "gamma2", "confound_label")])

# initialize list to store performance metrics for each condition
summary_list <- vector("list", nrow(conditions))

# loop over each condition to compute performance metrics
for (k in seq_len(nrow(conditions))) {
  # subset results for current condition
  res <- subset(
    sim_results,
    n == conditions$n[k] &
      gamma2 == conditions$gamma2[k]
  )

  # absolute performance metrics using {simhelpers}
  # includes bias, variance, MSE, and RMSE (with MCSEs)
  perf_abs <- calc_absolute(
    data = res,
    estimates = est,
    true_param = true_param,
    criteria = c("bias", "variance", "mse", "rmse")
  )

  # relative performance metrics
  # includes relative bias and relative RMSE (with MCSEs)
  perf_rel <- calc_relative(
    data = res,
    estimates = est,
    true_param = true_param,
    criteria = c("relative bias", "relative rmse")
  )

  # coverage and CI width metrics
  # evaluates 95% CI performance (with MCSEs)
  perf_cov <- calc_coverage(
    data = res,
    lower_bound = lo,
    upper_bound = hi,
    true_param = true_param,
    criteria = c("coverage", "width")
  )

  # combine condition info with all performance metrics
  summary_list[[k]] <- cbind(
    conditions[k, ],
    true_ace = unique(res$true_param),
    perf_abs,
    perf_rel,
    perf_cov
  )
}

# all conditions into final summary data frame
performance_summary <- do.call(rbind, summary_list)
```

```{r}
#| lst-label: lst-fig-performance
#| lst-cap: "Code for generating the performance metrics visualization"
#| label: fig-performance
#| fig-cap: "Performance of ACE estimator across sample sizes and confounding severity. Panel A shows relative bias, Panel B shows relative RMSE, Panel C shows coverage probability of 95% confidence intervals (dashed line at nominal 0.95 level), and Panel D shows average confidence interval width. Results demonstrate that model misspecification induces systematic bias that persists across sample sizes, while increasing sample size improves precision but not accuracy under misspecification."
#| fig-width: 10
#| fig-height: 8
#| echo: false
#| warning: false

# panel A: absolute bias plot (not used in final figure)
# shows raw difference between estimated and true ACE
p_bias <- ggplot(
  performance_summary,
  aes(x = n, y = bias, color = confound_label, group = confound_label)
) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  geom_point(size = 3) +
  geom_line() +
  labs(
    x = "Sample size (n)",
    y = "Absolute Bias",
    color = "Confounding\nnon-linearity",
    title = "Absolute Bias from Omitting X²"
  ) +
  theme_minimal() +
  theme(legend.position = "right")

# panel A (final): relative bias plot
# ratio of estimated to true ACE (1 = unbiased)
p_rel_bias <- ggplot(
  performance_summary,
  aes(x = n, y = rel_bias, color = confound_label, group = confound_label)
) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray50") +
  geom_point(size = 3) +
  geom_line() +
  labs(
    x = "Sample size (n)",
    y = "Relative Bias",
    color = "Confounding\nnon-linearity",
    title = "Relative Bias from Omitting X²"
  ) +
  theme_minimal() +
  theme(legend.position = "right")

# absolute RMSE plot (not used in final figure)
# raw root mean squared error
p_rmse <- ggplot(
  performance_summary,
  aes(x = n, y = rmse, color = confound_label, group = confound_label)
) +
  geom_point(size = 3) +
  geom_line() +
  labs(
    x = "Sample size (n)",
    y = "RMSE of ACE estimator",
    color = "Confounding\nnon-linearity",
    title = "Absolute RMSE of ACE Estimation"
  ) +
  theme_minimal() +
  theme(legend.position = "right")

# panel B: relative RMSE plot
# RMSE scaled by true parameter value
p_rel_rmse <- ggplot(
  performance_summary,
  aes(x = n, y = rel_rmse, color = confound_label, group = confound_label)
) +
  geom_point(size = 3) +
  geom_line() +
  labs(
    x = "Sample size (n)",
    y = "Relative RMSE",
    color = "Confounding\nnon-linearity",
    title = "Relative Accuracy of ACE Estimation"
  ) +
  theme_minimal() +
  theme(legend.position = "right")

# panel C: coverage probability plot
# proportion of 95% CIs containing the true ACE
# dashed line at 0.95 = nominal coverage level
p_coverage <- ggplot(
  performance_summary,
  aes(x = n, y = coverage, color = confound_label, group = confound_label)
) +
  geom_hline(yintercept = 0.95, linetype = "dashed", color = "gray50") +
  geom_point(size = 3) +
  geom_line() +
  labs(
    x = "Sample size (n)",
    y = "Coverage (95% CI)",
    color = "Confounding\nnon-linearity",
    title = "Coverage Under Residual Confounding"
  ) +
  coord_cartesian(ylim = c(0, 1)) +
  theme_minimal() +
  theme(legend.position = "right")

# panel D: confidence interval width plot
# average width of 95% CIs (measures precision)
p_width <- ggplot(
  performance_summary,
  aes(x = n, y = width, color = confound_label, group = confound_label)
) +
  geom_point(size = 3) +
  geom_line() +
  labs(
    x = "Sample size (n)",
    y = "Average CI width",
    color = "Confounding\nnon-linearity",
    title = "Precision of CI"
  ) +
  theme_minimal() +
  theme(legend.position = "right")

# combine plots into 2x2 grid with shared legend using {cowplot}
# extract legend from one plot to use as shared legend
legend <- get_legend(
  p_rel_bias + theme(legend.position = "right")
)

# arrange panels and legend side by side
plot_grid(
  plot_grid(p_rel_bias, p_rel_rmse, p_coverage, p_width, ncol = 2, nrow = 2),
  legend,
  rel_widths = c(3, 0.4)
)
```

```{r}
#| lst-label: lst-tbl-results
#| lst-cap: "Code for generating the performance metrics table"
#| label: tbl-results
#| tbl-cap: "Performance metrics for ACE estimator across simulation conditions. Values in parentheses are Monte Carlo standard errors (MCSE)."
#| echo: false

# prepare data for display table
# select relevant columns including estimates and their MCSEs
results_display <- performance_summary %>%
  select(n, confound_label, rel_bias, rel_bias_mcse, rel_rmse, rel_rmse_mcse,
         coverage, coverage_mcse, width, width_mcse) %>%
  mutate(
    # rename columns for display
    `Sample Size` = n,
    `Confounding` = confound_label,
    # format each metric as "estimate (MCSE)" with 3 decimal places
    `Relative Bias` = sprintf("%.3f (%.3f)", rel_bias, rel_bias_mcse),
    `Relative RMSE` = sprintf("%.3f (%.3f)", rel_rmse, rel_rmse_mcse),
    `Coverage` = sprintf("%.3f (%.3f)", coverage, coverage_mcse),
    `CI Width` = sprintf("%.3f (%.3f)", width, width_mcse)
  ) %>%
  # only the formatted columns for final display
  select(
    `Sample Size`,
    `Confounding`,
    `Relative Bias`,
    `Relative RMSE`,
    `Coverage`,
    `CI Width`
  )

# render table 
kable(results_display, align = "lccccc", row.names = FALSE)
```

### Reproducing the Simulation and Results

As previously mentioned, researchers may prefer not to use literate programming,
or embedding the simulation within a dynamic document may be impractical. In
this case, one could still follow the same steps shown thus far focusing only on
the .R files while still benefiting from a reproducible computational
environment. For example, `03_run_simulation.R` begins by
loading the required packages and sourcing other scripts that are needed:

```{r}
#| lst-label: lst-run-simulation
#| lst-cap: "Code for running simulation"
#| eval: false
#| echo: true

library(marginaleffects)
...

# Source helper functions                                                   
source("Simulation_Scripts/01_data_generation.R")                    
...

```

As an illustration, if we were to focus solely on the computational 
reproducibility of the code underlying the simulation and results, the 
environment specification would be considerably simpler 
(@lst-rix-env-simulation):

::: {#lst-rix-env-simulation}
\small

```r
rix(
  date = "2026-01-14",
  r_pkgs = c(
    "simhelpers", "ggplot2", "doParallel", "doRNG", "cowplot", "dplyr", 
    "rvinecopulib", "marginaleffects"
  ),
  ide = "rstudio",
  project_path = ".",
  overwrite = TRUE
)
```

Environment specification for simulation-only computational reproducibility
\normalsize
:::

After following Steps 1-3 with this simpler specification, the simulation
study may be reproduced within the Nix shell (@lst-run-simulation) as follows:

::: {#lst-run-simulation}
\small

``` bash
[nix-shell:~/AMPPS/Why-risk-it-when-you-can-rix-it]$ 
  Rscript Simulation_Scripts/03_run_simulation.R
```

Running the complete simulation workflow \normalsize
:::

In the same way, we could proceed with `04_performance_metrics.R` 
(@lst-run-performance), which loads the simulation
results (in `sim_results.rds`) and calculates the performance
metrics:

::: {#lst-run-performance}
\small

``` bash
[nix-shell:~/AMPPS/Why-risk-it-when-you-can-rix-it]$
  Rscript Simulation_Scripts/04_performance_metrics.R
```

Running the performance metrics calculation \normalsize
:::

Similarly, `05_plots.R` (@lst-run-plots) uses those saved metrics (in
`performance_summary.rds`) to create the plots:

::: {#lst-run-plots}
\small

``` bash
[nix-shell:~/AMPPS/Why-risk-it-when-you-can-rix-it]$
  Rscript Simulation_Scripts/05_plots.R
```

Generating the visualization plots \normalsize
:::

Therefore, the key advantage of executing within `nix-shell` is that all
dependencies (i.e., R version, packages, and system tools) match exactly those
specified in `default.nix`. Note, however, that this approach relies on manually 
running scripts in sequence. It ensures a reproducible environment but does 
not formalize the workflow itself; dependencies between scripts remain 
implicit in the code rather than explicitly declared.

## Additional Considerations for Advanced Workflows

Thus far, we have presented Nix and {rix} as standalone solutions for
computational reproducibility, contrasting initially them with tools like
{renv}, Docker, and {targets}. However, these tools are not mutually exclusive,
as in many cases, they can complement each other [@rodrigues_baumann_2025].
Additionally, as highlighted in previous literature [@peikert_brandmaier_2021;
@siepe_etall_2024; @piccolo_frampton_2016], fully reproducible research benefits
not only from a stable computational environment but also from explicit workflow
orchestration. We therefore briefly introduce {rixpress}, which extends
Nix-based reproducibility to formalized, multi-language pipelines. These topics
are not covered in depth; our goal is simply to clarify how these tools relate
to one another and orient readers toward resources for more advanced use cases.

### Workflow Orchestration: {targets} and {rixpress}

Complex simulation studies often benefit from workflow management systems that
track dependencies between computational steps, cache intermediate results, and
enable selective re-execution when inputs change. Two complementary approaches
exist within the Nix ecosystem.

#### Using {targets} Within Nix

The {targets} package [@landau_2021] provides workflow orchestration for R-based
projects. To integrate {targets} with Nix, include `targets` in the `r_pkgs`
parameter of `rix()` and execute the pipeline within `nix-shell` using
`Rscript -e 'targets::tar_make()'`. A shell hook can also be added via the
`shell_hook` argument to run the pipeline automatically when entering the Nix
shell. This approach is ideal for projects that remain within R and do not
require different environments for different pipeline steps (see {rix}
documentation:
<https://docs.ropensci.org/rix/articles/reproducible-pipelines.html>).

#### Using {rixpress} for Multi-Language Pipelines

The {rixpress} package [@rixpress], a sister package to {rix}, uses Nix itself
as the build automation tool rather than operating within a Nix environment.
Each pipeline step becomes a Nix derivation, built in isolation with automatic
caching based on content. The key advantage emerges in multi-language workflows:
different steps can execute in different Nix-defined environments (e.g., one
step using a specific version of R, another using Python, another using Julia).
The interface, inspired by {targets}, uses functions like `rxp_r()`, `rxp_py()`,
and `rxp_jl()` to define pipeline steps (see {rixpress} documentation:
<https://docs.ropensci.org/rixpress/articles/intro-concepts.html>). The GitHub
repository for this article directs interested readers to a demonstration of
{rixpress} applied to this entire project.  

### Converting Existing {renv} Projects

Researchers with existing {renv} projects can migrate using the `renv2nix()`
function, which reads an `renv.lock` file and generates an equivalent Nix
expression. This is particularly valuable for projects where {renv} encountered
system dependency issues or where stricter reproducibility guarantees are
desired. Unlike {renv}, which captures R package versions but not the R
interpreter or system libraries, Nix manages all layers of the software stack
(see {rix} documentation:
<https://docs.ropensci.org/rix/articles/renv2nix.html>).

### Containerization with Docker

Nix and Docker are not necessarily mutually exclusive
[@rodrigues_baumann_2026_polyglot]. Researchers already using Docker do not need
to abandon it to benefit from Nix—the two can be combined by using Nix inside
Docker containers to handle environment setup [@rodrigues_baumann_2025]. This is
particularly useful for deployment to cloud platforms or high-performance
computing clusters where Docker is standard but Nix may not be available (see
{rix} documentation:
<https://docs.ropensci.org/rix/articles/nix-inside-docker.html>).

# Discussion

Reproducibility in computational research is often treated as a matter of
transparency—making data and code available. This tutorial has argued that
transparency alone is insufficient without the ability to reliably reconstruct
the computational environments in which analyses are executed. For simulation
studies in particular, where results depend critically on software versions,
system libraries, and random number generation, environment-level
reproducibility is not optional but essential.

By introducing Nix and the {rix} package, we demonstrated a practical and
accessible approach to fully specifying and rebuilding computational
environments for simulation-based research. This approach enables analyses and
manuscripts to be rerun identically across machines and over time, transforming
reproducibility from an aspirational goal into a verifiable property of the
research workflow.

Importantly, adopting environment reproducibility does not require abandoning
existing analytic practices. Nix is agnostic to programming language, editor,
workflow structure, and manuscript template, allowing researchers to retain
familiar tools while strengthening the reliability of their work. In this sense,
reproducible environments serve as enabling infrastructure—supporting, rather
than replacing, other best practices such as version control, workflow
orchestration, and transparent reporting.

If reproducibility is to function as a cornerstone of cumulative science, then
the ability to reconstruct computational environments must become a routine part
of methodological practice. Tools such as Nix and {rix} lower the barrier to
achieving this goal, making fully reproducible simulation research feasible
without requiring deep systems expertise. We hope this tutorial helps normalize
environment-level reproducibility as a standard component of rigorous
computational research in psychology and beyond.

{{< pagebreak >}}

# References

::: {#refs}
:::

{{< include appendix_a.qmd >}}

{{< include appendix_b.qmd >}}
